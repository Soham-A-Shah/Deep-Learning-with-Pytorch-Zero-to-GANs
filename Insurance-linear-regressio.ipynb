{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Insurance cost prediction using linear regression\n",
    "\n",
    "In this assignment we're going to use information like a person's age, sex, BMI, no. of children and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from: https://www.kaggle.com/mirichoi0218/insurance\n",
    "\n",
    "\n",
    "We will create a model with the following steps:\n",
    "1. Download and explore the dataset\n",
    "2. Prepare the dataset for training\n",
    "3. Create a linear regression model\n",
    "4. Train the model to fit the data\n",
    "5. Make predictions using the trained model\n",
    "\n",
    "\n",
    "This assignment builds upon the concepts from the first 2 lectures. It will help to review these Jupyter notebooks:\n",
    "- PyTorch basics: https://jovian.ml/aakashns/01-pytorch-basics\n",
    "- Linear Regression: https://jovian.ml/aakashns/02-linear-regression\n",
    "- Logistic Regression: https://jovian.ml/aakashns/03-logistic-regression\n",
    "- Linear regression (minimal): https://jovian.ml/aakashns/housing-linear-minimal\n",
    "- Logistic regression (minimal): https://jovian.ml/aakashns/mnist-logistic-minimal\n",
    "\n",
    "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end . In some cases, you'll be required to choose some hyperparameters (learning rate, batch size etc.). Try to experiment with the hypeparameters to get the lowest loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run the commands below if imports fail\n",
    "# !conda install numpy pytorch torchvision cpuonly -c pytorch -y\n",
    "# !pip install matplotlib --upgrade --quiet\n",
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import jovian\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='02-insurance-linear-regression' # will be used by jovian.commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and explore the data\n",
    "\n",
    "Let us begin by downloading the data. We'll use the `download_url` function from PyTorch to get the data as a CSV (comma-separated values) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv to ./insurance.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d149f9ac1943d591b0c705a8829ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_URL = \"https://hub.jovian.ml/wp-content/uploads/2020/05/insurance.csv\"\n",
    "DATA_FILENAME = \"insurance.csv\"\n",
    "download_url(DATASET_URL, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset into memory, we'll use the `read_csv` function from the `pandas` library. The data will be loaded as a Pandas dataframe. See this short tutorial to learn more: https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
    "dataframe_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a slight customization of the data, so that you every participant receives a slightly different version of the dataset. Fill in your name below as a string (enter at least 5 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'SS141299' # at least 5 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `customize_dataset` function will customize the dataset slightly using your name as a source of random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_dataset(dataframe_raw, rand_str):\n",
    "    dataframe = dataframe_raw.copy(deep=True)\n",
    "    # drop some rows\n",
    "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
    "    # scale input\n",
    "    dataframe.bmi = dataframe.bmi * ord(rand_str[1])/100.\n",
    "    # scale target\n",
    "    dataframe.charges = dataframe.charges * ord(rand_str[2])/100.\n",
    "    # drop column\n",
    "    if ord(rand_str[3]) % 2 == 1:\n",
    "        dataframe = dataframe.drop(['region'], axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>43</td>\n",
       "      <td>male</td>\n",
       "      <td>31.58980</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>southeast</td>\n",
       "      <td>20854.610896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>19.47595</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>1571.180761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>37</td>\n",
       "      <td>female</td>\n",
       "      <td>28.88400</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>19519.894310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>63</td>\n",
       "      <td>female</td>\n",
       "      <td>23.02420</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northeast</td>\n",
       "      <td>14466.351144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>23.33960</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "      <td>10521.514612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex       bmi  children smoker     region       charges\n",
       "665    43    male  31.58980         2    yes  southeast  20854.610896\n",
       "579    25  female  19.47595         0     no  northeast   1571.180761\n",
       "84     37  female  28.88400         2    yes  southwest  19519.894310\n",
       "244    63  female  23.02420         0    yes  northeast  14466.351144\n",
       "1307   32    male  23.33960         4    yes  northwest  10521.514612"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = customize_dataset(dataframe_raw, your_name)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us answer some basic questions about the dataset. \n",
    "\n",
    "\n",
    "**Q: How many rows does the dataset have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1271\n"
     ]
    }
   ],
   "source": [
    "num_rows = dataframe.shape[0]\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How many columns doe the dataset have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "num_cols = dataframe.shape[1]\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of the input variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'children', 'smoker', 'region']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = list(dataframe.columns[:-1])\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Which of the input columns are non-numeric or categorial variables ?**\n",
    "\n",
    "Hint: `sex` is one of them. List the columns that are not numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = list(dataframe.select_dtypes(include=['object']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the column titles of output/target variable(s)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = ['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: (Optional) What is the minimum, maximum and average value of the `charges` column? Can you show the distribution of values in a graph?**\n",
    "Use this data visualization cheatsheet for referece: https://jovian.ml/aakashns/dataviz-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Value: 549.718211\n",
      "Maximum Value: 31247.5097249\n",
      "Average Value: 6558.669986868289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffb82afad10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your answer here\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Minimum Value:\",dataframe['charges'].min())\n",
    "print(\"Maximum Value:\",dataframe['charges'].max())\n",
    "print(\"Average Value:\",dataframe['charges'].mean())\n",
    "\n",
    "\n",
    "plt.title(\"Distribution of Insuarance Charges Values\")\n",
    "sns.distplot(dataframe.charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to commit your notebook to Jovian after every step, so that you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Please enter your API key ( from https://jovian.ml/ ):\u001b[0m\n",
      "API KEY: ········\n",
      "[jovian] Updating notebook \"soham-a-shah/02-insurance-linear-regression\" on https://jovian.ml/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ml/soham-a-shah/02-insurance-linear-regression\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/soham-a-shah/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the dataset for training\n",
    "\n",
    "We need to convert the data from the Pandas dataframe into a PyTorch tensors for training. To do this, the first step is to convert it numpy arrays. If you've filled out `input_cols`, `categorial_cols` and `output_cols` correctly, this following function will perform the conversion to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Convert non-numeric categorical columns to numbers\n",
    "    for col in categorical_cols:\n",
    "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1[input_cols].to_numpy()\n",
    "    targets_array = dataframe1[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) to understand how we're converting categorical variables into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[43.     ,  1.     , 31.5898 ,  2.     ,  1.     ,  2.     ],\n",
       "        [25.     ,  0.     , 19.47595,  0.     ,  0.     ,  0.     ],\n",
       "        [37.     ,  0.     , 28.884  ,  2.     ,  1.     ,  3.     ],\n",
       "        ...,\n",
       "        [47.     ,  1.     , 21.0903 ,  1.     ,  1.     ,  2.     ],\n",
       "        [35.     ,  1.     , 20.0279 ,  1.     ,  0.     ,  1.     ],\n",
       "        [43.     ,  0.     , 20.9741 ,  1.     ,  1.     ,  0.     ]]),\n",
       " array([[20854.610896 ],\n",
       "        [ 1571.1807615],\n",
       "        [19519.89431  ],\n",
       "        ...,\n",
       "        [10769.551681 ],\n",
       "        [ 2511.355693 ],\n",
       "        [10667.957727 ]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
    "inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Convert the numpy arrays `inputs_array` and `targets_array` into PyTorch tensors. Make sure that the data type is `torch.float32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs_array, dtype=torch.float32)\n",
    "targets = torch.tensor(targets_array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype, targets.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a `TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Pick a number between `0.1` and `0.2` to determine the fraction of data that will be used for creating the validation set. Then use `random_split` to create training & validation datasets. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "val_percent = (0.1+0.2)/2 # between 0.1 and 0.2\n",
    "print(val_percent)\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create data loaders for training & validation.\n",
    "\n",
    "**Q: Pick a batch size for the data loader.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a batch of data to verify everything is working fine so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[56.0000,  0.0000, 22.0780,  1.0000,  0.0000,  1.0000],\n",
      "        [25.0000,  0.0000, 34.9679,  1.0000,  0.0000,  2.0000],\n",
      "        [23.0000,  0.0000, 35.4825,  1.0000,  1.0000,  0.0000],\n",
      "        [58.0000,  0.0000, 27.3610,  0.0000,  0.0000,  0.0000],\n",
      "        [60.0000,  0.0000, 29.8841,  0.0000,  0.0000,  0.0000],\n",
      "        [41.0000,  1.0000, 19.8702,  1.0000,  0.0000,  0.0000],\n",
      "        [51.0000,  1.0000, 20.2645,  4.0000,  0.0000,  1.0000],\n",
      "        [49.0000,  1.0000, 25.1490,  0.0000,  0.0000,  3.0000],\n",
      "        [31.0000,  1.0000, 16.9320,  0.0000,  0.0000,  3.0000],\n",
      "        [62.0000,  1.0000, 26.6513,  0.0000,  0.0000,  0.0000],\n",
      "        [47.0000,  0.0000, 20.0030,  1.0000,  0.0000,  3.0000],\n",
      "        [53.0000,  1.0000, 23.9704,  0.0000,  0.0000,  1.0000],\n",
      "        [63.0000,  0.0000, 20.8164,  0.0000,  0.0000,  1.0000],\n",
      "        [56.0000,  0.0000, 23.4973,  0.0000,  0.0000,  0.0000],\n",
      "        [30.0000,  1.0000, 26.0620,  1.0000,  0.0000,  3.0000],\n",
      "        [55.0000,  1.0000, 30.9590,  0.0000,  0.0000,  3.0000],\n",
      "        [28.0000,  1.0000, 22.3934,  2.0000,  0.0000,  0.0000],\n",
      "        [49.0000,  1.0000, 26.8090,  3.0000,  0.0000,  1.0000],\n",
      "        [34.0000,  0.0000, 15.7700,  3.0000,  0.0000,  0.0000],\n",
      "        [63.0000,  0.0000, 31.2910,  0.0000,  1.0000,  3.0000],\n",
      "        [49.0000,  0.0000, 19.7913,  3.0000,  1.0000,  0.0000],\n",
      "        [28.0000,  0.0000, 22.0033,  2.0000,  0.0000,  2.0000],\n",
      "        [35.0000,  0.0000, 28.3943,  1.0000,  0.0000,  2.0000],\n",
      "        [62.0000,  0.0000, 25.3109,  2.0000,  0.0000,  1.0000],\n",
      "        [61.0000,  1.0000, 19.6336,  0.0000,  0.0000,  0.0000],\n",
      "        [42.0000,  1.0000, 22.3270,  0.0000,  0.0000,  3.0000],\n",
      "        [37.0000,  1.0000, 25.6262,  3.0000,  0.0000,  1.0000],\n",
      "        [51.0000,  1.0000, 29.8551,  1.0000,  0.0000,  2.0000],\n",
      "        [22.0000,  1.0000, 32.7850,  0.0000,  0.0000,  3.0000],\n",
      "        [49.0000,  0.0000, 28.8591,  1.0000,  0.0000,  1.0000],\n",
      "        [53.0000,  1.0000, 28.3071,  0.0000,  1.0000,  0.0000],\n",
      "        [61.0000,  0.0000, 32.4530,  2.0000,  0.0000,  3.0000]])\n",
      "targets: tensor([[ 5901.7275],\n",
      "        [ 1586.8335],\n",
      "        [20043.0586],\n",
      "        [ 6091.1670],\n",
      "        [ 6482.1348],\n",
      "        [ 3360.6550],\n",
      "        [ 5644.8491],\n",
      "        [ 3977.1731],\n",
      "        [ 1597.4976],\n",
      "        [ 6641.9526],\n",
      "        [12855.9238],\n",
      "        [ 4836.2070],\n",
      "        [ 6984.7578],\n",
      "        [ 5712.2822],\n",
      "        [ 1793.0796],\n",
      "        [10108.8389],\n",
      "        [ 2173.1960],\n",
      "        [ 5032.0352],\n",
      "        [ 3308.9885],\n",
      "        [23923.9805],\n",
      "        [11812.3867],\n",
      "        [ 2126.8162],\n",
      "        [ 2570.1611],\n",
      "        [ 7359.6826],\n",
      "        [ 6433.5059],\n",
      "        [ 2925.1643],\n",
      "        [ 3330.4629],\n",
      "        [ 4599.2192],\n",
      "        [  824.4725],\n",
      "        [ 4696.1079],\n",
      "        [21194.6641],\n",
      "        [ 6975.1851]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(\"inputs:\", xb)\n",
    "    print(\"targets:\", yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work by committing to Jovian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"soham-a-shah/02-insurance-linear-regression\" on https://jovian.ml/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ml/soham-a-shah/02-insurance-linear-regression\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/soham-a-shah/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Linear Regression Model\n",
    "\n",
    "Our model itself is a fairly straightforward linear regression (we'll build more complex models in the next assignment). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Complete the class definition below by filling out the constructor (`__init__`), `forward`, `training_step` and `validation_step` methods.**\n",
    "\n",
    "Hint: Think carefully about picking a good loss fuction (it's not cross entropy). Maybe try 2-3 of them and see which one works best. See https://pytorch.org/docs/stable/nn.functional.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear(xb)                          # fill this\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch \n",
    "        # Generate predictions\n",
    "        out = self(inputs)          \n",
    "        # Calcuate loss\n",
    "        loss = F.l1_loss(out,targets)                           # fill this\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out,targets)                            # fill this    \n",
    "        return {'val_loss': loss.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 20th epoch\n",
    "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
    "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a model using the `InsuranceModel` class. You may need to come back later and re-run the next cell to reinitialize the model, in case the loss becomes `nan` or `infinity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InsuranceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the weights and biases of the model using `model.parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0699,  0.2393, -0.0625,  0.0327, -0.2641,  0.0116]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0055], requires_grad=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final commit before we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"soham-a-shah/02-insurance-linear-regression\" on https://jovian.ml/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ml/soham-a-shah/02-insurance-linear-regression\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/soham-a-shah/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model to fit the data\n",
    "\n",
    "To train our model, we'll use the same `fit` function explained in the lecture. That's the benefit of defining a generic training loop - you can use it for any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Use the `evaluate` function to calculate the loss on the validation set before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 5814.63427734375}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model, val_loader) # Use the the evaluate function\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now ready to train the model. You may need to run the training loop many times, for different number of epochs and with different learning rates, to get a good result. Also, if your loss becomes too large (or `nan`), you may have to re-initialize the model by running the cell `model = InsuranceModel()`. Experiment with this for a while, and try to get to as low a loss as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Train the model 4-5 times with different learning rates & for different number of epochs.**\n",
    "\n",
    "Hint: Vary learning rates by orders of 10 (e.g. `1e-2`, `1e-3`, `1e-4`, `1e-5`, `1e-6`) to figure out what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 3060.6240\n",
      "Epoch [40], val_loss: 3003.0254\n",
      "Epoch [60], val_loss: 2951.2937\n",
      "Epoch [80], val_loss: 2909.7102\n",
      "Epoch [100], val_loss: 2882.8362\n",
      "Epoch [120], val_loss: 2865.3020\n",
      "Epoch [140], val_loss: 2859.4199\n",
      "Epoch [160], val_loss: 2855.6555\n",
      "Epoch [180], val_loss: 2852.8289\n",
      "Epoch [200], val_loss: 2851.1360\n",
      "Epoch [220], val_loss: 2850.6533\n",
      "Epoch [240], val_loss: 2849.9260\n",
      "Epoch [260], val_loss: 2849.3274\n",
      "Epoch [280], val_loss: 2847.3943\n",
      "Epoch [300], val_loss: 2847.1296\n",
      "Epoch [320], val_loss: 2845.9092\n",
      "Epoch [340], val_loss: 2844.5579\n",
      "Epoch [360], val_loss: 2843.5176\n",
      "Epoch [380], val_loss: 2843.3027\n",
      "Epoch [400], val_loss: 2842.2263\n",
      "Epoch [420], val_loss: 2840.9958\n",
      "Epoch [440], val_loss: 2840.6562\n",
      "Epoch [460], val_loss: 2839.3379\n",
      "Epoch [480], val_loss: 2838.8923\n",
      "Epoch [500], val_loss: 2838.8318\n",
      "Epoch [520], val_loss: 2838.1570\n",
      "Epoch [540], val_loss: 2836.6091\n",
      "Epoch [560], val_loss: 2836.0369\n",
      "Epoch [580], val_loss: 2835.0906\n",
      "Epoch [600], val_loss: 2833.9119\n",
      "Epoch [620], val_loss: 2834.3293\n",
      "Epoch [640], val_loss: 2833.7302\n",
      "Epoch [660], val_loss: 2832.7441\n",
      "Epoch [680], val_loss: 2831.8518\n",
      "Epoch [700], val_loss: 2831.4490\n",
      "Epoch [720], val_loss: 2831.1738\n",
      "Epoch [740], val_loss: 2829.7820\n",
      "Epoch [760], val_loss: 2829.2937\n",
      "Epoch [780], val_loss: 2827.8601\n",
      "Epoch [800], val_loss: 2827.7949\n",
      "Epoch [820], val_loss: 2827.5959\n",
      "Epoch [840], val_loss: 2826.3748\n",
      "Epoch [860], val_loss: 2825.2366\n",
      "Epoch [880], val_loss: 2824.8467\n",
      "Epoch [900], val_loss: 2824.1619\n",
      "Epoch [920], val_loss: 2823.1492\n",
      "Epoch [940], val_loss: 2823.1980\n",
      "Epoch [960], val_loss: 2821.9414\n",
      "Epoch [980], val_loss: 2822.0264\n",
      "Epoch [1000], val_loss: 2821.2520\n"
     ]
    }
   ],
   "source": [
    "model = InsuranceModel()\n",
    "epochs = 1000\n",
    "lr = 1e-2\n",
    "history1 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 2821.0452\n",
      "Epoch [40], val_loss: 2821.0496\n",
      "Epoch [60], val_loss: 2820.9963\n",
      "Epoch [80], val_loss: 2820.9434\n",
      "Epoch [100], val_loss: 2820.8704\n",
      "Epoch [120], val_loss: 2820.8408\n",
      "Epoch [140], val_loss: 2820.8079\n",
      "Epoch [160], val_loss: 2820.7644\n",
      "Epoch [180], val_loss: 2820.6975\n",
      "Epoch [200], val_loss: 2820.6414\n",
      "Epoch [220], val_loss: 2820.6379\n",
      "Epoch [240], val_loss: 2820.5725\n",
      "Epoch [260], val_loss: 2820.4473\n",
      "Epoch [280], val_loss: 2820.4602\n",
      "Epoch [300], val_loss: 2820.3477\n",
      "Epoch [320], val_loss: 2820.3840\n",
      "Epoch [340], val_loss: 2820.2773\n",
      "Epoch [360], val_loss: 2820.2146\n",
      "Epoch [380], val_loss: 2820.1536\n",
      "Epoch [400], val_loss: 2820.0745\n",
      "Epoch [420], val_loss: 2820.0037\n",
      "Epoch [440], val_loss: 2819.9304\n",
      "Epoch [460], val_loss: 2819.8865\n",
      "Epoch [480], val_loss: 2819.8064\n",
      "Epoch [500], val_loss: 2819.7371\n",
      "Epoch [520], val_loss: 2819.7969\n",
      "Epoch [540], val_loss: 2819.6133\n",
      "Epoch [560], val_loss: 2819.5740\n",
      "Epoch [580], val_loss: 2819.5481\n",
      "Epoch [600], val_loss: 2819.4778\n",
      "Epoch [620], val_loss: 2819.3977\n",
      "Epoch [640], val_loss: 2819.4099\n",
      "Epoch [660], val_loss: 2819.3049\n",
      "Epoch [680], val_loss: 2819.3105\n",
      "Epoch [700], val_loss: 2819.2043\n",
      "Epoch [720], val_loss: 2819.1396\n",
      "Epoch [740], val_loss: 2819.0916\n",
      "Epoch [760], val_loss: 2819.1013\n",
      "Epoch [780], val_loss: 2818.9983\n",
      "Epoch [800], val_loss: 2818.9883\n",
      "Epoch [820], val_loss: 2818.8479\n",
      "Epoch [840], val_loss: 2818.8757\n",
      "Epoch [860], val_loss: 2818.8054\n",
      "Epoch [880], val_loss: 2818.7180\n",
      "Epoch [900], val_loss: 2818.6887\n",
      "Epoch [920], val_loss: 2818.6621\n",
      "Epoch [940], val_loss: 2818.6719\n",
      "Epoch [960], val_loss: 2818.5520\n",
      "Epoch [980], val_loss: 2818.5769\n",
      "Epoch [1000], val_loss: 2818.4514\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "history2 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 2818.4514\n",
      "Epoch [40], val_loss: 2818.4500\n",
      "Epoch [60], val_loss: 2818.4414\n",
      "Epoch [80], val_loss: 2818.4387\n",
      "Epoch [100], val_loss: 2818.4343\n",
      "Epoch [120], val_loss: 2818.4304\n",
      "Epoch [140], val_loss: 2818.4167\n",
      "Epoch [160], val_loss: 2818.4231\n",
      "Epoch [180], val_loss: 2818.4150\n",
      "Epoch [200], val_loss: 2818.4050\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-4\n",
    "history3 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 2818.4045\n",
      "Epoch [40], val_loss: 2818.4050\n",
      "Epoch [60], val_loss: 2818.4036\n",
      "Epoch [80], val_loss: 2818.4036\n",
      "Epoch [100], val_loss: 2818.4036\n",
      "Epoch [120], val_loss: 2818.4031\n",
      "Epoch [140], val_loss: 2818.4041\n",
      "Epoch [160], val_loss: 2818.4026\n",
      "Epoch [180], val_loss: 2818.4014\n",
      "Epoch [200], val_loss: 2818.4006\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-5\n",
    "history4 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], val_loss: 2818.4011\n",
      "Epoch [40], val_loss: 2818.4011\n",
      "Epoch [60], val_loss: 2818.4014\n",
      "Epoch [80], val_loss: 2818.4016\n",
      "Epoch [100], val_loss: 2818.4016\n",
      "Epoch [120], val_loss: 2818.4021\n",
      "Epoch [140], val_loss: 2818.4021\n",
      "Epoch [160], val_loss: 2818.4023\n",
      "Epoch [180], val_loss: 2818.4021\n",
      "Epoch [200], val_loss: 2818.4016\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-6\n",
    "history5 = fit(epochs, lr, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the final validation loss of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'val_loss': 5814.63427734375}, {'val_loss': 5102.68359375}, {'val_loss': 4405.56689453125}, {'val_loss': 3863.559814453125}, {'val_loss': 3470.628173828125}, {'val_loss': 3245.951171875}, {'val_loss': 3123.042724609375}, {'val_loss': 3081.451904296875}, {'val_loss': 3068.4365234375}, {'val_loss': 3068.197265625}, {'val_loss': 3071.600341796875}, {'val_loss': 3073.2177734375}, {'val_loss': 3074.34765625}, {'val_loss': 3075.583251953125}, {'val_loss': 3074.291748046875}, {'val_loss': 3073.637939453125}, {'val_loss': 3070.767822265625}, {'val_loss': 3069.148681640625}, {'val_loss': 3065.780517578125}, {'val_loss': 3063.411376953125}, {'val_loss': 3060.6240234375}, {'val_loss': 3056.946044921875}, {'val_loss': 3054.550537109375}, {'val_loss': 3051.399169921875}, {'val_loss': 3046.959716796875}, {'val_loss': 3044.045654296875}, {'val_loss': 3040.6474609375}, {'val_loss': 3038.143798828125}, {'val_loss': 3035.006103515625}, {'val_loss': 3032.122802734375}, {'val_loss': 3029.9423828125}, {'val_loss': 3026.632568359375}, {'val_loss': 3024.148193359375}, {'val_loss': 3021.0576171875}, {'val_loss': 3018.363525390625}, {'val_loss': 3016.600830078125}, {'val_loss': 3013.810791015625}, {'val_loss': 3011.334716796875}, {'val_loss': 3008.673828125}, {'val_loss': 3006.3251953125}, {'val_loss': 3003.025390625}, {'val_loss': 3000.102294921875}, {'val_loss': 2996.725830078125}, {'val_loss': 2993.481689453125}, {'val_loss': 2991.3115234375}, {'val_loss': 2988.1865234375}, {'val_loss': 2986.494873046875}, {'val_loss': 2983.932861328125}, {'val_loss': 2981.896484375}, {'val_loss': 2978.598876953125}, {'val_loss': 2975.657470703125}, {'val_loss': 2972.9013671875}, {'val_loss': 2970.434326171875}, {'val_loss': 2967.3740234375}, {'val_loss': 2964.404296875}, {'val_loss': 2961.996826171875}, {'val_loss': 2960.0751953125}, {'val_loss': 2957.737548828125}, {'val_loss': 2955.730712890625}, {'val_loss': 2953.1298828125}, {'val_loss': 2951.293701171875}, {'val_loss': 2949.06640625}, {'val_loss': 2946.449951171875}, {'val_loss': 2943.348388671875}, {'val_loss': 2941.427734375}, {'val_loss': 2938.814208984375}, {'val_loss': 2937.197021484375}, {'val_loss': 2935.280517578125}, {'val_loss': 2933.697998046875}, {'val_loss': 2931.326171875}, {'val_loss': 2928.973876953125}, {'val_loss': 2927.03125}, {'val_loss': 2924.42578125}, {'val_loss': 2922.953857421875}, {'val_loss': 2920.765625}, {'val_loss': 2919.5791015625}, {'val_loss': 2918.00390625}, {'val_loss': 2915.269775390625}, {'val_loss': 2912.873291015625}, {'val_loss': 2912.080078125}, {'val_loss': 2909.710205078125}, {'val_loss': 2909.330810546875}, {'val_loss': 2906.69140625}, {'val_loss': 2904.680419921875}, {'val_loss': 2902.340576171875}, {'val_loss': 2901.722412109375}, {'val_loss': 2899.95703125}, {'val_loss': 2898.103759765625}, {'val_loss': 2896.714599609375}, {'val_loss': 2895.4140625}, {'val_loss': 2893.617431640625}, {'val_loss': 2893.13671875}, {'val_loss': 2891.663330078125}, {'val_loss': 2889.476318359375}, {'val_loss': 2888.133544921875}, {'val_loss': 2886.949462890625}, {'val_loss': 2884.8671875}, {'val_loss': 2884.379638671875}, {'val_loss': 2883.920166015625}, {'val_loss': 2883.690673828125}, {'val_loss': 2882.836181640625}, {'val_loss': 2881.295166015625}, {'val_loss': 2879.845703125}, {'val_loss': 2879.099365234375}, {'val_loss': 2878.933349609375}, {'val_loss': 2877.7783203125}, {'val_loss': 2875.9775390625}, {'val_loss': 2876.145751953125}, {'val_loss': 2874.474609375}, {'val_loss': 2873.099853515625}, {'val_loss': 2872.0927734375}, {'val_loss': 2871.678466796875}, {'val_loss': 2870.869873046875}, {'val_loss': 2869.900634765625}, {'val_loss': 2870.040771484375}, {'val_loss': 2869.494873046875}, {'val_loss': 2868.343017578125}, {'val_loss': 2867.7724609375}, {'val_loss': 2866.947509765625}, {'val_loss': 2866.44140625}, {'val_loss': 2865.302001953125}, {'val_loss': 2865.208984375}, {'val_loss': 2864.9072265625}, {'val_loss': 2865.0302734375}, {'val_loss': 2864.955322265625}, {'val_loss': 2864.611083984375}, {'val_loss': 2864.042724609375}, {'val_loss': 2863.572265625}, {'val_loss': 2863.213134765625}, {'val_loss': 2862.936767578125}, {'val_loss': 2862.519775390625}, {'val_loss': 2862.161865234375}, {'val_loss': 2861.3603515625}, {'val_loss': 2860.645751953125}, {'val_loss': 2860.7578125}, {'val_loss': 2859.828125}, {'val_loss': 2859.873291015625}, {'val_loss': 2859.993896484375}, {'val_loss': 2859.939697265625}, {'val_loss': 2859.26171875}, {'val_loss': 2859.419921875}, {'val_loss': 2859.054931640625}, {'val_loss': 2858.1201171875}, {'val_loss': 2857.869873046875}, {'val_loss': 2858.285400390625}, {'val_loss': 2857.776123046875}, {'val_loss': 2857.53125}, {'val_loss': 2857.166015625}, {'val_loss': 2857.779541015625}, {'val_loss': 2857.740478515625}, {'val_loss': 2858.896484375}, {'val_loss': 2858.1298828125}, {'val_loss': 2857.903076171875}, {'val_loss': 2857.691162109375}, {'val_loss': 2856.789306640625}, {'val_loss': 2857.919677734375}, {'val_loss': 2856.3291015625}, {'val_loss': 2855.8974609375}, {'val_loss': 2854.91015625}, {'val_loss': 2856.123046875}, {'val_loss': 2855.655517578125}, {'val_loss': 2855.208251953125}, {'val_loss': 2855.452392578125}, {'val_loss': 2855.692138671875}, {'val_loss': 2855.705078125}, {'val_loss': 2853.489990234375}, {'val_loss': 2854.168212890625}, {'val_loss': 2853.838623046875}, {'val_loss': 2854.251953125}, {'val_loss': 2855.006103515625}, {'val_loss': 2854.961669921875}, {'val_loss': 2854.778564453125}, {'val_loss': 2854.281005859375}, {'val_loss': 2854.0888671875}, {'val_loss': 2854.103271484375}, {'val_loss': 2852.932861328125}, {'val_loss': 2852.438720703125}, {'val_loss': 2852.888916015625}, {'val_loss': 2854.400146484375}, {'val_loss': 2853.842041015625}, {'val_loss': 2852.828857421875}, {'val_loss': 2853.531494140625}, {'val_loss': 2853.842529296875}, {'val_loss': 2852.767578125}, {'val_loss': 2853.7568359375}, {'val_loss': 2853.34765625}, {'val_loss': 2853.377197265625}, {'val_loss': 2853.812744140625}, {'val_loss': 2853.3359375}, {'val_loss': 2853.162109375}, {'val_loss': 2852.256591796875}, {'val_loss': 2852.559814453125}, {'val_loss': 2852.817626953125}, {'val_loss': 2853.195556640625}, {'val_loss': 2852.2587890625}, {'val_loss': 2852.524658203125}, {'val_loss': 2852.763916015625}, {'val_loss': 2853.388916015625}, {'val_loss': 2851.9248046875}, {'val_loss': 2851.75390625}, {'val_loss': 2851.135986328125}, {'val_loss': 2850.855224609375}, {'val_loss': 2851.337646484375}, {'val_loss': 2851.379638671875}, {'val_loss': 2852.052734375}, {'val_loss': 2851.4970703125}, {'val_loss': 2851.189697265625}, {'val_loss': 2851.2802734375}, {'val_loss': 2850.71484375}, {'val_loss': 2851.501953125}, {'val_loss': 2851.676513671875}, {'val_loss': 2851.721923828125}, {'val_loss': 2851.291015625}, {'val_loss': 2850.827880859375}, {'val_loss': 2850.824951171875}, {'val_loss': 2851.4462890625}, {'val_loss': 2850.725341796875}, {'val_loss': 2850.319091796875}, {'val_loss': 2850.317138671875}, {'val_loss': 2850.947509765625}, {'val_loss': 2850.6533203125}, {'val_loss': 2850.691162109375}, {'val_loss': 2850.512939453125}, {'val_loss': 2850.307861328125}, {'val_loss': 2849.497314453125}, {'val_loss': 2849.5400390625}, {'val_loss': 2849.3388671875}, {'val_loss': 2849.502197265625}, {'val_loss': 2849.815673828125}, {'val_loss': 2850.321533203125}, {'val_loss': 2850.2568359375}, {'val_loss': 2850.012939453125}, {'val_loss': 2850.109375}, {'val_loss': 2849.969482421875}, {'val_loss': 2850.309814453125}, {'val_loss': 2849.664794921875}, {'val_loss': 2848.9033203125}, {'val_loss': 2848.9580078125}, {'val_loss': 2849.923095703125}, {'val_loss': 2849.841552734375}, {'val_loss': 2849.926025390625}, {'val_loss': 2848.639892578125}, {'val_loss': 2848.980712890625}, {'val_loss': 2849.216796875}, {'val_loss': 2849.4921875}, {'val_loss': 2849.8251953125}, {'val_loss': 2849.014404296875}, {'val_loss': 2848.593505859375}, {'val_loss': 2848.87890625}, {'val_loss': 2848.548095703125}, {'val_loss': 2848.9873046875}, {'val_loss': 2848.746826171875}, {'val_loss': 2848.8828125}, {'val_loss': 2848.845458984375}, {'val_loss': 2848.983642578125}, {'val_loss': 2848.694580078125}, {'val_loss': 2848.7294921875}, {'val_loss': 2849.078125}, {'val_loss': 2849.408935546875}, {'val_loss': 2849.1171875}, {'val_loss': 2849.327392578125}, {'val_loss': 2848.655029296875}, {'val_loss': 2848.2216796875}, {'val_loss': 2848.316650390625}, {'val_loss': 2847.992919921875}, {'val_loss': 2847.689697265625}, {'val_loss': 2847.797119140625}, {'val_loss': 2847.964111328125}, {'val_loss': 2848.435302734375}, {'val_loss': 2848.348388671875}, {'val_loss': 2848.144287109375}, {'val_loss': 2847.850830078125}, {'val_loss': 2847.181640625}, {'val_loss': 2847.259521484375}, {'val_loss': 2847.739990234375}, {'val_loss': 2847.936279296875}, {'val_loss': 2847.672119140625}, {'val_loss': 2847.775634765625}, {'val_loss': 2847.735595703125}, {'val_loss': 2847.545654296875}, {'val_loss': 2847.394287109375}, {'val_loss': 2847.517578125}, {'val_loss': 2847.667724609375}, {'val_loss': 2847.495849609375}, {'val_loss': 2847.620849609375}, {'val_loss': 2847.240234375}, {'val_loss': 2847.429931640625}, {'val_loss': 2847.597412109375}, {'val_loss': 2847.454345703125}, {'val_loss': 2847.29296875}, {'val_loss': 2847.423828125}, {'val_loss': 2847.417236328125}, {'val_loss': 2847.271484375}, {'val_loss': 2847.630615234375}, {'val_loss': 2847.513671875}, {'val_loss': 2847.382080078125}, {'val_loss': 2847.302001953125}, {'val_loss': 2847.295166015625}, {'val_loss': 2847.241455078125}, {'val_loss': 2847.422119140625}, {'val_loss': 2847.129638671875}, {'val_loss': 2847.0625}, {'val_loss': 2847.160888671875}, {'val_loss': 2847.09375}, {'val_loss': 2847.215087890625}, {'val_loss': 2846.749755859375}, {'val_loss': 2846.830078125}, {'val_loss': 2846.691162109375}, {'val_loss': 2846.436279296875}, {'val_loss': 2846.399169921875}, {'val_loss': 2846.411376953125}, {'val_loss': 2846.324462890625}, {'val_loss': 2846.00390625}, {'val_loss': 2845.966552734375}, {'val_loss': 2845.8212890625}, {'val_loss': 2845.737060546875}, {'val_loss': 2846.197998046875}, {'val_loss': 2846.267333984375}, {'val_loss': 2846.2734375}, {'val_loss': 2846.147705078125}, {'val_loss': 2845.9091796875}, {'val_loss': 2846.063232421875}, {'val_loss': 2845.867919921875}, {'val_loss': 2845.898681640625}, {'val_loss': 2845.765380859375}, {'val_loss': 2845.863525390625}, {'val_loss': 2845.91796875}, {'val_loss': 2845.7685546875}, {'val_loss': 2845.677734375}, {'val_loss': 2845.640380859375}, {'val_loss': 2845.496826171875}, {'val_loss': 2845.1865234375}, {'val_loss': 2845.022705078125}, {'val_loss': 2844.550048828125}, {'val_loss': 2844.560546875}, {'val_loss': 2844.620361328125}, {'val_loss': 2844.95703125}, {'val_loss': 2844.986083984375}, {'val_loss': 2844.954345703125}, {'val_loss': 2844.586181640625}, {'val_loss': 2844.557861328125}, {'val_loss': 2844.525634765625}, {'val_loss': 2844.305908203125}, {'val_loss': 2844.213134765625}, {'val_loss': 2844.7412109375}, {'val_loss': 2845.276611328125}, {'val_loss': 2844.612548828125}, {'val_loss': 2844.347900390625}, {'val_loss': 2844.532470703125}, {'val_loss': 2844.219482421875}, {'val_loss': 2844.131591796875}, {'val_loss': 2843.961181640625}, {'val_loss': 2844.2080078125}, {'val_loss': 2844.2861328125}, {'val_loss': 2844.1025390625}, {'val_loss': 2843.8564453125}, {'val_loss': 2844.069091796875}, {'val_loss': 2844.1865234375}, {'val_loss': 2843.9443359375}, {'val_loss': 2843.7275390625}, {'val_loss': 2843.517578125}, {'val_loss': 2844.083251953125}, {'val_loss': 2843.974609375}, {'val_loss': 2843.9033203125}, {'val_loss': 2843.724853515625}, {'val_loss': 2843.4296875}, {'val_loss': 2843.709716796875}, {'val_loss': 2843.5859375}, {'val_loss': 2843.436767578125}, {'val_loss': 2843.375732421875}, {'val_loss': 2843.098876953125}, {'val_loss': 2842.988037109375}, {'val_loss': 2842.741455078125}, {'val_loss': 2842.9267578125}, {'val_loss': 2842.787353515625}, {'val_loss': 2842.696533203125}, {'val_loss': 2842.3740234375}, {'val_loss': 2842.6181640625}, {'val_loss': 2843.558837890625}, {'val_loss': 2843.568359375}, {'val_loss': 2843.302734375}, {'val_loss': 2843.383544921875}, {'val_loss': 2843.1494140625}, {'val_loss': 2842.852783203125}, {'val_loss': 2842.669189453125}, {'val_loss': 2842.787841796875}, {'val_loss': 2843.057373046875}, {'val_loss': 2842.58984375}, {'val_loss': 2842.102783203125}, {'val_loss': 2841.728515625}, {'val_loss': 2842.202392578125}, {'val_loss': 2842.146240234375}, {'val_loss': 2842.0556640625}, {'val_loss': 2842.302490234375}, {'val_loss': 2842.047607421875}, {'val_loss': 2841.834228515625}, {'val_loss': 2841.540771484375}, {'val_loss': 2842.250732421875}, {'val_loss': 2843.174072265625}, {'val_loss': 2843.2919921875}, {'val_loss': 2842.226318359375}, {'val_loss': 2842.416259765625}, {'val_loss': 2842.0869140625}, {'val_loss': 2842.135009765625}, {'val_loss': 2842.252197265625}, {'val_loss': 2842.200927734375}, {'val_loss': 2842.401611328125}, {'val_loss': 2841.927001953125}, {'val_loss': 2841.704345703125}, {'val_loss': 2841.644287109375}, {'val_loss': 2841.987060546875}, {'val_loss': 2842.4775390625}, {'val_loss': 2842.213134765625}, {'val_loss': 2841.799560546875}, {'val_loss': 2841.560546875}, {'val_loss': 2841.2734375}, {'val_loss': 2841.1572265625}, {'val_loss': 2841.2783203125}, {'val_loss': 2841.2265625}, {'val_loss': 2840.914306640625}, {'val_loss': 2840.995849609375}, {'val_loss': 2841.125732421875}, {'val_loss': 2841.193359375}, {'val_loss': 2841.162109375}, {'val_loss': 2841.40234375}, {'val_loss': 2841.251220703125}, {'val_loss': 2840.996826171875}, {'val_loss': 2841.148193359375}, {'val_loss': 2841.013427734375}, {'val_loss': 2840.785888671875}, {'val_loss': 2840.75}, {'val_loss': 2840.57421875}, {'val_loss': 2840.510986328125}, {'val_loss': 2840.897705078125}, {'val_loss': 2840.911865234375}, {'val_loss': 2840.636474609375}, {'val_loss': 2841.2451171875}, {'val_loss': 2840.9921875}, {'val_loss': 2841.152587890625}, {'val_loss': 2840.679931640625}, {'val_loss': 2840.65625}, {'val_loss': 2840.935791015625}, {'val_loss': 2840.9697265625}, {'val_loss': 2840.605224609375}, {'val_loss': 2840.3291015625}, {'val_loss': 2840.566650390625}, {'val_loss': 2840.4970703125}, {'val_loss': 2840.361328125}, {'val_loss': 2839.9951171875}, {'val_loss': 2840.985107421875}, {'val_loss': 2840.669189453125}, {'val_loss': 2840.392822265625}, {'val_loss': 2839.8408203125}, {'val_loss': 2839.751708984375}, {'val_loss': 2840.309814453125}, {'val_loss': 2840.717041015625}, {'val_loss': 2840.458984375}, {'val_loss': 2840.138671875}, {'val_loss': 2839.870361328125}, {'val_loss': 2839.70703125}, {'val_loss': 2839.337890625}, {'val_loss': 2839.916259765625}, {'val_loss': 2839.6357421875}, {'val_loss': 2839.718505859375}, {'val_loss': 2839.279052734375}, {'val_loss': 2840.1650390625}, {'val_loss': 2839.8525390625}, {'val_loss': 2840.110107421875}, {'val_loss': 2839.867919921875}, {'val_loss': 2839.280517578125}, {'val_loss': 2839.251220703125}, {'val_loss': 2839.976318359375}, {'val_loss': 2839.600830078125}, {'val_loss': 2838.764892578125}, {'val_loss': 2839.291015625}, {'val_loss': 2838.853515625}, {'val_loss': 2838.7646484375}, {'val_loss': 2839.1552734375}, {'val_loss': 2838.8642578125}, {'val_loss': 2838.693359375}, {'val_loss': 2838.892333984375}, {'val_loss': 2839.073486328125}, {'val_loss': 2839.058349609375}, {'val_loss': 2839.134521484375}, {'val_loss': 2839.1025390625}, {'val_loss': 2840.307373046875}, {'val_loss': 2839.6884765625}, {'val_loss': 2839.506591796875}, {'val_loss': 2839.149169921875}, {'val_loss': 2838.6328125}, {'val_loss': 2838.985595703125}, {'val_loss': 2838.482666015625}, {'val_loss': 2838.825927734375}, {'val_loss': 2839.092529296875}, {'val_loss': 2839.371337890625}, {'val_loss': 2838.502685546875}, {'val_loss': 2837.964599609375}, {'val_loss': 2837.639892578125}, {'val_loss': 2838.105712890625}, {'val_loss': 2838.083740234375}, {'val_loss': 2838.831787109375}, {'val_loss': 2838.2958984375}, {'val_loss': 2838.3759765625}, {'val_loss': 2838.2353515625}, {'val_loss': 2838.126220703125}, {'val_loss': 2838.030029296875}, {'val_loss': 2838.424072265625}, {'val_loss': 2838.186279296875}, {'val_loss': 2838.305419921875}, {'val_loss': 2837.826171875}, {'val_loss': 2838.439453125}, {'val_loss': 2837.591552734375}, {'val_loss': 2837.6669921875}, {'val_loss': 2837.488037109375}, {'val_loss': 2837.493896484375}, {'val_loss': 2837.001220703125}, {'val_loss': 2837.825927734375}, {'val_loss': 2838.50390625}, {'val_loss': 2837.9736328125}, {'val_loss': 2838.241455078125}, {'val_loss': 2838.156982421875}, {'val_loss': 2837.736328125}, {'val_loss': 2837.372802734375}, {'val_loss': 2837.902099609375}, {'val_loss': 2837.250244140625}, {'val_loss': 2837.212158203125}, {'val_loss': 2837.130615234375}, {'val_loss': 2837.256591796875}, {'val_loss': 2836.7099609375}, {'val_loss': 2836.509033203125}, {'val_loss': 2836.887451171875}, {'val_loss': 2837.748779296875}, {'val_loss': 2837.4072265625}, {'val_loss': 2837.588623046875}, {'val_loss': 2836.727783203125}, {'val_loss': 2836.962890625}, {'val_loss': 2836.993896484375}, {'val_loss': 2837.268310546875}, {'val_loss': 2837.140869140625}, {'val_loss': 2836.530029296875}, {'val_loss': 2836.609130859375}, {'val_loss': 2836.650390625}, {'val_loss': 2836.465087890625}, {'val_loss': 2836.753173828125}, {'val_loss': 2836.445068359375}, {'val_loss': 2836.691650390625}, {'val_loss': 2836.871826171875}, {'val_loss': 2836.532958984375}, {'val_loss': 2836.180908203125}, {'val_loss': 2836.8212890625}, {'val_loss': 2836.569091796875}, {'val_loss': 2836.614501953125}, {'val_loss': 2835.988525390625}, {'val_loss': 2835.962158203125}, {'val_loss': 2836.196533203125}, {'val_loss': 2836.175537109375}, {'val_loss': 2836.036865234375}, {'val_loss': 2836.556396484375}, {'val_loss': 2836.572265625}, {'val_loss': 2835.967529296875}, {'val_loss': 2836.036865234375}, {'val_loss': 2836.231201171875}, {'val_loss': 2835.999755859375}, {'val_loss': 2836.419677734375}, {'val_loss': 2836.16015625}, {'val_loss': 2836.206298828125}, {'val_loss': 2836.077392578125}, {'val_loss': 2835.699462890625}, {'val_loss': 2835.860595703125}, {'val_loss': 2836.163330078125}, {'val_loss': 2836.155517578125}, {'val_loss': 2836.396240234375}, {'val_loss': 2836.214599609375}, {'val_loss': 2836.166748046875}, {'val_loss': 2835.805419921875}, {'val_loss': 2835.6650390625}, {'val_loss': 2835.2724609375}, {'val_loss': 2835.432861328125}, {'val_loss': 2835.290771484375}, {'val_loss': 2835.255859375}, {'val_loss': 2835.090576171875}, {'val_loss': 2835.5419921875}, {'val_loss': 2834.7724609375}, {'val_loss': 2835.296630859375}, {'val_loss': 2835.419189453125}, {'val_loss': 2835.516357421875}, {'val_loss': 2834.7841796875}, {'val_loss': 2834.850830078125}, {'val_loss': 2834.8369140625}, {'val_loss': 2834.521240234375}, {'val_loss': 2834.494140625}, {'val_loss': 2834.8837890625}, {'val_loss': 2835.150146484375}, {'val_loss': 2835.4814453125}, {'val_loss': 2835.200439453125}, {'val_loss': 2834.533203125}, {'val_loss': 2834.982421875}, {'val_loss': 2834.806640625}, {'val_loss': 2834.195556640625}, {'val_loss': 2834.294921875}, {'val_loss': 2833.911865234375}, {'val_loss': 2835.104248046875}, {'val_loss': 2835.169189453125}, {'val_loss': 2835.335205078125}, {'val_loss': 2835.2705078125}, {'val_loss': 2834.476318359375}, {'val_loss': 2834.449951171875}, {'val_loss': 2834.860595703125}, {'val_loss': 2834.333984375}, {'val_loss': 2834.83203125}, {'val_loss': 2834.9326171875}, {'val_loss': 2834.989013671875}, {'val_loss': 2834.748046875}, {'val_loss': 2834.571533203125}, {'val_loss': 2835.040283203125}, {'val_loss': 2834.802734375}, {'val_loss': 2834.177001953125}, {'val_loss': 2833.591064453125}, {'val_loss': 2833.739501953125}, {'val_loss': 2833.704833984375}, {'val_loss': 2834.329345703125}, {'val_loss': 2834.317138671875}, {'val_loss': 2834.864501953125}, {'val_loss': 2835.205322265625}, {'val_loss': 2834.635986328125}, {'val_loss': 2834.298828125}, {'val_loss': 2834.110107421875}, {'val_loss': 2834.532958984375}, {'val_loss': 2833.8857421875}, {'val_loss': 2834.2529296875}, {'val_loss': 2834.031982421875}, {'val_loss': 2834.070068359375}, {'val_loss': 2834.075439453125}, {'val_loss': 2833.304443359375}, {'val_loss': 2833.608642578125}, {'val_loss': 2833.920654296875}, {'val_loss': 2834.030517578125}, {'val_loss': 2833.199951171875}, {'val_loss': 2833.1962890625}, {'val_loss': 2832.801513671875}, {'val_loss': 2833.730224609375}, {'val_loss': 2833.553955078125}, {'val_loss': 2833.800537109375}, {'val_loss': 2833.11328125}, {'val_loss': 2833.1826171875}, {'val_loss': 2832.725830078125}, {'val_loss': 2832.713623046875}, {'val_loss': 2832.722412109375}, {'val_loss': 2833.28515625}, {'val_loss': 2833.5263671875}, {'val_loss': 2832.9443359375}, {'val_loss': 2833.05859375}, {'val_loss': 2832.296142578125}, {'val_loss': 2833.042724609375}, {'val_loss': 2832.2734375}, {'val_loss': 2832.370849609375}, {'val_loss': 2832.0732421875}, {'val_loss': 2832.146484375}, {'val_loss': 2832.613525390625}, {'val_loss': 2832.804931640625}, {'val_loss': 2832.744140625}, {'val_loss': 2832.515869140625}, {'val_loss': 2832.580810546875}, {'val_loss': 2832.235107421875}, {'val_loss': 2832.140625}, {'val_loss': 2832.203857421875}, {'val_loss': 2832.109619140625}, {'val_loss': 2832.167236328125}, {'val_loss': 2831.7548828125}, {'val_loss': 2832.153076171875}, {'val_loss': 2832.1103515625}, {'val_loss': 2832.070556640625}, {'val_loss': 2831.4619140625}, {'val_loss': 2831.6337890625}, {'val_loss': 2831.554931640625}, {'val_loss': 2832.259521484375}, {'val_loss': 2832.908447265625}, {'val_loss': 2833.2744140625}, {'val_loss': 2832.163330078125}, {'val_loss': 2831.790283203125}, {'val_loss': 2831.851806640625}, {'val_loss': 2831.3818359375}, {'val_loss': 2831.402099609375}, {'val_loss': 2831.201171875}, {'val_loss': 2830.746826171875}, {'val_loss': 2831.406982421875}, {'val_loss': 2831.64453125}, {'val_loss': 2831.332275390625}, {'val_loss': 2831.76953125}, {'val_loss': 2831.020751953125}, {'val_loss': 2830.896240234375}, {'val_loss': 2831.564453125}, {'val_loss': 2831.595703125}, {'val_loss': 2831.495361328125}, {'val_loss': 2831.787841796875}, {'val_loss': 2831.542236328125}, {'val_loss': 2831.3994140625}, {'val_loss': 2831.274169921875}, {'val_loss': 2832.222412109375}, {'val_loss': 2831.768798828125}, {'val_loss': 2831.448974609375}, {'val_loss': 2831.6484375}, {'val_loss': 2831.33203125}, {'val_loss': 2830.975830078125}, {'val_loss': 2830.6806640625}, {'val_loss': 2830.998779296875}, {'val_loss': 2831.528076171875}, {'val_loss': 2831.1806640625}, {'val_loss': 2832.458984375}, {'val_loss': 2831.201416015625}, {'val_loss': 2831.0712890625}, {'val_loss': 2830.880615234375}, {'val_loss': 2831.3798828125}, {'val_loss': 2829.989501953125}, {'val_loss': 2829.978271484375}, {'val_loss': 2830.58203125}, {'val_loss': 2830.768310546875}, {'val_loss': 2831.074462890625}, {'val_loss': 2830.803466796875}, {'val_loss': 2830.7626953125}, {'val_loss': 2831.173828125}, {'val_loss': 2831.083251953125}, {'val_loss': 2829.9677734375}, {'val_loss': 2829.748291015625}, {'val_loss': 2830.080810546875}, {'val_loss': 2830.268798828125}, {'val_loss': 2830.98046875}, {'val_loss': 2831.017578125}, {'val_loss': 2830.507080078125}, {'val_loss': 2830.08984375}, {'val_loss': 2830.543701171875}, {'val_loss': 2829.7763671875}, {'val_loss': 2830.460205078125}, {'val_loss': 2829.3310546875}, {'val_loss': 2829.517578125}, {'val_loss': 2830.1826171875}, {'val_loss': 2830.460693359375}, {'val_loss': 2829.999267578125}, {'val_loss': 2830.00390625}, {'val_loss': 2830.30859375}, {'val_loss': 2829.781982421875}, {'val_loss': 2829.676025390625}, {'val_loss': 2829.3876953125}, {'val_loss': 2829.333740234375}, {'val_loss': 2829.574951171875}, {'val_loss': 2829.156982421875}, {'val_loss': 2829.5283203125}, {'val_loss': 2828.99609375}, {'val_loss': 2828.74609375}, {'val_loss': 2829.311767578125}, {'val_loss': 2829.363037109375}, {'val_loss': 2828.841064453125}, {'val_loss': 2829.5849609375}, {'val_loss': 2829.4755859375}, {'val_loss': 2829.370361328125}, {'val_loss': 2828.810546875}, {'val_loss': 2829.0703125}, {'val_loss': 2829.364501953125}, {'val_loss': 2828.869140625}, {'val_loss': 2829.462646484375}, {'val_loss': 2829.293701171875}, {'val_loss': 2829.117919921875}, {'val_loss': 2829.248046875}, {'val_loss': 2828.8857421875}, {'val_loss': 2829.012451171875}, {'val_loss': 2828.778564453125}, {'val_loss': 2828.685791015625}, {'val_loss': 2828.501953125}, {'val_loss': 2828.3857421875}, {'val_loss': 2827.971435546875}, {'val_loss': 2828.406982421875}, {'val_loss': 2828.701904296875}, {'val_loss': 2829.0771484375}, {'val_loss': 2828.764404296875}, {'val_loss': 2828.942626953125}, {'val_loss': 2828.291015625}, {'val_loss': 2828.511474609375}, {'val_loss': 2828.840087890625}, {'val_loss': 2828.604736328125}, {'val_loss': 2828.5869140625}, {'val_loss': 2827.860107421875}, {'val_loss': 2828.009521484375}, {'val_loss': 2827.9677734375}, {'val_loss': 2827.9033203125}, {'val_loss': 2827.93359375}, {'val_loss': 2828.512451171875}, {'val_loss': 2828.198486328125}, {'val_loss': 2827.5048828125}, {'val_loss': 2827.513427734375}, {'val_loss': 2827.7587890625}, {'val_loss': 2827.219482421875}, {'val_loss': 2827.707275390625}, {'val_loss': 2827.825927734375}, {'val_loss': 2827.705810546875}, {'val_loss': 2828.194091796875}, {'val_loss': 2828.055419921875}, {'val_loss': 2828.20703125}, {'val_loss': 2827.8095703125}, {'val_loss': 2828.147216796875}, {'val_loss': 2827.772705078125}, {'val_loss': 2827.794921875}, {'val_loss': 2827.044189453125}, {'val_loss': 2827.220703125}, {'val_loss': 2826.78515625}, {'val_loss': 2827.384521484375}, {'val_loss': 2827.142822265625}, {'val_loss': 2827.504150390625}, {'val_loss': 2826.976806640625}, {'val_loss': 2827.26171875}, {'val_loss': 2827.457275390625}, {'val_loss': 2827.587158203125}, {'val_loss': 2828.076171875}, {'val_loss': 2827.785888671875}, {'val_loss': 2827.367431640625}, {'val_loss': 2826.831298828125}, {'val_loss': 2827.378662109375}, {'val_loss': 2827.0654296875}, {'val_loss': 2827.224609375}, {'val_loss': 2827.191162109375}, {'val_loss': 2827.56640625}, {'val_loss': 2827.595947265625}, {'val_loss': 2827.640625}, {'val_loss': 2826.676513671875}, {'val_loss': 2826.9599609375}, {'val_loss': 2826.501220703125}, {'val_loss': 2826.668701171875}, {'val_loss': 2827.234619140625}, {'val_loss': 2826.62890625}, {'val_loss': 2826.8603515625}, {'val_loss': 2826.68359375}, {'val_loss': 2826.92578125}, {'val_loss': 2827.222412109375}, {'val_loss': 2827.0537109375}, {'val_loss': 2827.105224609375}, {'val_loss': 2826.427490234375}, {'val_loss': 2826.9462890625}, {'val_loss': 2826.190673828125}, {'val_loss': 2826.820068359375}, {'val_loss': 2826.864990234375}, {'val_loss': 2826.7431640625}, {'val_loss': 2826.374755859375}, {'val_loss': 2826.567626953125}, {'val_loss': 2826.37109375}, {'val_loss': 2825.8046875}, {'val_loss': 2825.8701171875}, {'val_loss': 2826.483154296875}, {'val_loss': 2825.560791015625}, {'val_loss': 2825.217041015625}, {'val_loss': 2825.135986328125}, {'val_loss': 2827.036376953125}, {'val_loss': 2827.015625}, {'val_loss': 2826.231201171875}, {'val_loss': 2825.990234375}, {'val_loss': 2825.450439453125}, {'val_loss': 2825.719482421875}, {'val_loss': 2826.339599609375}, {'val_loss': 2825.560546875}, {'val_loss': 2825.511474609375}, {'val_loss': 2825.190673828125}, {'val_loss': 2825.638916015625}, {'val_loss': 2825.236572265625}, {'val_loss': 2825.340576171875}, {'val_loss': 2824.736083984375}, {'val_loss': 2824.847412109375}, {'val_loss': 2825.4365234375}, {'val_loss': 2825.587890625}, {'val_loss': 2824.891357421875}, {'val_loss': 2824.904541015625}, {'val_loss': 2825.042236328125}, {'val_loss': 2824.473388671875}, {'val_loss': 2824.490234375}, {'val_loss': 2824.521728515625}, {'val_loss': 2824.5498046875}, {'val_loss': 2824.483642578125}, {'val_loss': 2824.750244140625}, {'val_loss': 2824.955810546875}, {'val_loss': 2824.6572265625}, {'val_loss': 2825.462158203125}, {'val_loss': 2825.064453125}, {'val_loss': 2824.821044921875}, {'val_loss': 2824.8466796875}, {'val_loss': 2825.247314453125}, {'val_loss': 2824.875732421875}, {'val_loss': 2824.584716796875}, {'val_loss': 2824.4794921875}, {'val_loss': 2824.806884765625}, {'val_loss': 2824.569580078125}, {'val_loss': 2824.54296875}, {'val_loss': 2824.565673828125}, {'val_loss': 2824.606689453125}, {'val_loss': 2824.9638671875}, {'val_loss': 2824.194091796875}, {'val_loss': 2824.296630859375}, {'val_loss': 2824.281494140625}, {'val_loss': 2823.993408203125}, {'val_loss': 2823.132080078125}, {'val_loss': 2823.3505859375}, {'val_loss': 2823.599609375}, {'val_loss': 2824.055908203125}, {'val_loss': 2824.538330078125}, {'val_loss': 2824.161865234375}, {'val_loss': 2823.885009765625}, {'val_loss': 2824.378173828125}, {'val_loss': 2823.685302734375}, {'val_loss': 2823.1533203125}, {'val_loss': 2824.168701171875}, {'val_loss': 2823.572998046875}, {'val_loss': 2823.893310546875}, {'val_loss': 2824.371826171875}, {'val_loss': 2823.962158203125}, {'val_loss': 2823.873779296875}, {'val_loss': 2824.252685546875}, {'val_loss': 2823.7275390625}, {'val_loss': 2824.2216796875}, {'val_loss': 2824.43359375}, {'val_loss': 2824.1865234375}, {'val_loss': 2823.483154296875}, {'val_loss': 2823.871337890625}, {'val_loss': 2823.373291015625}, {'val_loss': 2823.345947265625}, {'val_loss': 2823.149169921875}, {'val_loss': 2823.023681640625}, {'val_loss': 2823.540771484375}, {'val_loss': 2823.08984375}, {'val_loss': 2823.73046875}, {'val_loss': 2823.894775390625}, {'val_loss': 2823.380126953125}, {'val_loss': 2823.9853515625}, {'val_loss': 2823.470458984375}, {'val_loss': 2823.212158203125}, {'val_loss': 2823.5537109375}, {'val_loss': 2823.128662109375}, {'val_loss': 2823.101318359375}, {'val_loss': 2822.794677734375}, {'val_loss': 2822.572998046875}, {'val_loss': 2823.013671875}, {'val_loss': 2822.6396484375}, {'val_loss': 2822.3056640625}, {'val_loss': 2822.7353515625}, {'val_loss': 2823.202880859375}, {'val_loss': 2823.197998046875}, {'val_loss': 2823.379150390625}, {'val_loss': 2822.551513671875}, {'val_loss': 2822.483642578125}, {'val_loss': 2822.464111328125}, {'val_loss': 2821.952392578125}, {'val_loss': 2822.701904296875}, {'val_loss': 2822.367431640625}, {'val_loss': 2821.888916015625}, {'val_loss': 2822.510009765625}, {'val_loss': 2822.760986328125}, {'val_loss': 2822.197998046875}, {'val_loss': 2822.182373046875}, {'val_loss': 2822.271240234375}, {'val_loss': 2821.990966796875}, {'val_loss': 2821.4072265625}, {'val_loss': 2821.609375}, {'val_loss': 2821.7138671875}, {'val_loss': 2822.822998046875}, {'val_loss': 2822.6787109375}, {'val_loss': 2821.94140625}, {'val_loss': 2822.3017578125}, {'val_loss': 2821.984375}, {'val_loss': 2821.7041015625}, {'val_loss': 2822.098876953125}, {'val_loss': 2822.0068359375}, {'val_loss': 2821.9990234375}, {'val_loss': 2821.855224609375}, {'val_loss': 2821.612548828125}, {'val_loss': 2822.120849609375}, {'val_loss': 2822.0810546875}, {'val_loss': 2823.426513671875}, {'val_loss': 2822.5546875}, {'val_loss': 2822.416259765625}, {'val_loss': 2822.133544921875}, {'val_loss': 2822.16796875}, {'val_loss': 2822.0107421875}, {'val_loss': 2821.390380859375}, {'val_loss': 2822.1376953125}, {'val_loss': 2821.451416015625}, {'val_loss': 2822.0263671875}, {'val_loss': 2822.607666015625}, {'val_loss': 2822.148193359375}, {'val_loss': 2822.282958984375}, {'val_loss': 2822.037109375}, {'val_loss': 2821.653564453125}, {'val_loss': 2821.655517578125}, {'val_loss': 2821.055908203125}, {'val_loss': 2821.427490234375}, {'val_loss': 2821.242919921875}, {'val_loss': 2821.6240234375}, {'val_loss': 2820.658203125}, {'val_loss': 2820.903076171875}, {'val_loss': 2821.809326171875}, {'val_loss': 2821.1357421875}, {'val_loss': 2821.122314453125}, {'val_loss': 2821.12109375}, {'val_loss': 2821.9658203125}, {'val_loss': 2822.08203125}, {'val_loss': 2821.3330078125}, {'val_loss': 2821.251953125}, {'val_loss': 2821.23046875}, {'val_loss': 2821.177001953125}, {'val_loss': 2821.129150390625}, {'val_loss': 2821.108642578125}, {'val_loss': 2821.104736328125}, {'val_loss': 2821.117919921875}, {'val_loss': 2821.127685546875}, {'val_loss': 2821.102294921875}, {'val_loss': 2821.095703125}, {'val_loss': 2821.0673828125}, {'val_loss': 2821.0458984375}, {'val_loss': 2821.048828125}, {'val_loss': 2821.019775390625}, {'val_loss': 2821.036865234375}, {'val_loss': 2821.050537109375}, {'val_loss': 2821.058349609375}, {'val_loss': 2821.0625}, {'val_loss': 2821.041748046875}, {'val_loss': 2821.025390625}, {'val_loss': 2821.045166015625}, {'val_loss': 2821.033935546875}, {'val_loss': 2821.070068359375}, {'val_loss': 2821.078857421875}, {'val_loss': 2821.0576171875}, {'val_loss': 2821.047119140625}, {'val_loss': 2821.071533203125}, {'val_loss': 2821.052001953125}, {'val_loss': 2821.05078125}, {'val_loss': 2821.025634765625}, {'val_loss': 2821.0361328125}, {'val_loss': 2821.059326171875}, {'val_loss': 2821.036376953125}, {'val_loss': 2821.0517578125}, {'val_loss': 2821.031005859375}, {'val_loss': 2821.022705078125}, {'val_loss': 2821.039306640625}, {'val_loss': 2821.0234375}, {'val_loss': 2821.05859375}, {'val_loss': 2821.04296875}, {'val_loss': 2821.049560546875}, {'val_loss': 2821.062255859375}, {'val_loss': 2821.035400390625}, {'val_loss': 2821.015380859375}, {'val_loss': 2821.0302734375}, {'val_loss': 2821.021240234375}, {'val_loss': 2821.0205078125}, {'val_loss': 2821.040771484375}, {'val_loss': 2821.033203125}, {'val_loss': 2821.036376953125}, {'val_loss': 2821.052734375}, {'val_loss': 2821.042724609375}, {'val_loss': 2821.023193359375}, {'val_loss': 2820.994140625}, {'val_loss': 2821.010498046875}, {'val_loss': 2820.991455078125}, {'val_loss': 2821.017578125}, {'val_loss': 2820.987548828125}, {'val_loss': 2821.001953125}, {'val_loss': 2820.982421875}, {'val_loss': 2820.996337890625}, {'val_loss': 2820.982421875}, {'val_loss': 2820.978271484375}, {'val_loss': 2820.935546875}, {'val_loss': 2820.947021484375}, {'val_loss': 2820.947265625}, {'val_loss': 2820.931884765625}, {'val_loss': 2820.962158203125}, {'val_loss': 2820.9375}, {'val_loss': 2820.956298828125}, {'val_loss': 2820.954345703125}, {'val_loss': 2820.9833984375}, {'val_loss': 2820.9755859375}, {'val_loss': 2820.976318359375}, {'val_loss': 2820.979736328125}, {'val_loss': 2820.9345703125}, {'val_loss': 2820.898193359375}, {'val_loss': 2820.916748046875}, {'val_loss': 2820.931884765625}, {'val_loss': 2820.955078125}, {'val_loss': 2820.943359375}, {'val_loss': 2820.953857421875}, {'val_loss': 2820.931640625}, {'val_loss': 2820.978515625}, {'val_loss': 2820.961669921875}, {'val_loss': 2820.9541015625}, {'val_loss': 2820.958251953125}, {'val_loss': 2820.957275390625}, {'val_loss': 2820.9609375}, {'val_loss': 2820.9365234375}, {'val_loss': 2820.905517578125}, {'val_loss': 2820.895263671875}, {'val_loss': 2820.866943359375}, {'val_loss': 2820.873046875}, {'val_loss': 2820.8935546875}, {'val_loss': 2820.890380859375}, {'val_loss': 2820.916015625}, {'val_loss': 2820.846435546875}, {'val_loss': 2820.8564453125}, {'val_loss': 2820.866455078125}, {'val_loss': 2820.870361328125}, {'val_loss': 2820.841064453125}, {'val_loss': 2820.843994140625}, {'val_loss': 2820.84765625}, {'val_loss': 2820.83984375}, {'val_loss': 2820.825927734375}, {'val_loss': 2820.848388671875}, {'val_loss': 2820.8740234375}, {'val_loss': 2820.8857421875}, {'val_loss': 2820.887939453125}, {'val_loss': 2820.8984375}, {'val_loss': 2820.9091796875}, {'val_loss': 2820.871337890625}, {'val_loss': 2820.841796875}, {'val_loss': 2820.857666015625}, {'val_loss': 2820.802734375}, {'val_loss': 2820.806884765625}, {'val_loss': 2820.816650390625}, {'val_loss': 2820.841796875}, {'val_loss': 2820.872802734375}, {'val_loss': 2820.8408203125}, {'val_loss': 2820.8388671875}, {'val_loss': 2820.842529296875}, {'val_loss': 2820.878173828125}, {'val_loss': 2820.857666015625}, {'val_loss': 2820.85546875}, {'val_loss': 2820.858642578125}, {'val_loss': 2820.833740234375}, {'val_loss': 2820.795654296875}, {'val_loss': 2820.811767578125}, {'val_loss': 2820.828369140625}, {'val_loss': 2820.819091796875}, {'val_loss': 2820.833740234375}, {'val_loss': 2820.856689453125}, {'val_loss': 2820.832763671875}, {'val_loss': 2820.797119140625}, {'val_loss': 2820.782470703125}, {'val_loss': 2820.7734375}, {'val_loss': 2820.795166015625}, {'val_loss': 2820.812255859375}, {'val_loss': 2820.807861328125}, {'val_loss': 2820.804931640625}, {'val_loss': 2820.802734375}, {'val_loss': 2820.802734375}, {'val_loss': 2820.733154296875}, {'val_loss': 2820.73828125}, {'val_loss': 2820.75}, {'val_loss': 2820.725830078125}, {'val_loss': 2820.74609375}, {'val_loss': 2820.744873046875}, {'val_loss': 2820.760009765625}, {'val_loss': 2820.775390625}, {'val_loss': 2820.784423828125}, {'val_loss': 2820.8037109375}, {'val_loss': 2820.808837890625}, {'val_loss': 2820.766357421875}, {'val_loss': 2820.768310546875}, {'val_loss': 2820.741455078125}, {'val_loss': 2820.756103515625}, {'val_loss': 2820.767578125}, {'val_loss': 2820.764404296875}, {'val_loss': 2820.7734375}, {'val_loss': 2820.793701171875}, {'val_loss': 2820.783935546875}, {'val_loss': 2820.80859375}, {'val_loss': 2820.810791015625}, {'val_loss': 2820.780517578125}, {'val_loss': 2820.785888671875}, {'val_loss': 2820.805419921875}, {'val_loss': 2820.806396484375}, {'val_loss': 2820.801513671875}, {'val_loss': 2820.751708984375}, {'val_loss': 2820.7607421875}, {'val_loss': 2820.7734375}, {'val_loss': 2820.755615234375}, {'val_loss': 2820.740966796875}, {'val_loss': 2820.7724609375}, {'val_loss': 2820.721923828125}, {'val_loss': 2820.730712890625}, {'val_loss': 2820.738037109375}, {'val_loss': 2820.697509765625}, {'val_loss': 2820.682373046875}, {'val_loss': 2820.671630859375}, {'val_loss': 2820.687744140625}, {'val_loss': 2820.707763671875}, {'val_loss': 2820.712890625}, {'val_loss': 2820.743896484375}, {'val_loss': 2820.680419921875}, {'val_loss': 2820.672119140625}, {'val_loss': 2820.626953125}, {'val_loss': 2820.641845703125}, {'val_loss': 2820.6552734375}, {'val_loss': 2820.677978515625}, {'val_loss': 2820.666015625}, {'val_loss': 2820.694091796875}, {'val_loss': 2820.690185546875}, {'val_loss': 2820.66796875}, {'val_loss': 2820.674560546875}, {'val_loss': 2820.659912109375}, {'val_loss': 2820.6376953125}, {'val_loss': 2820.641357421875}, {'val_loss': 2820.633544921875}, {'val_loss': 2820.634765625}, {'val_loss': 2820.6552734375}, {'val_loss': 2820.659912109375}, {'val_loss': 2820.659423828125}, {'val_loss': 2820.634765625}, {'val_loss': 2820.615966796875}, {'val_loss': 2820.605224609375}, {'val_loss': 2820.62109375}, {'val_loss': 2820.649169921875}, {'val_loss': 2820.653564453125}, {'val_loss': 2820.665283203125}, {'val_loss': 2820.636474609375}, {'val_loss': 2820.60546875}, {'val_loss': 2820.628173828125}, {'val_loss': 2820.623291015625}, {'val_loss': 2820.628662109375}, {'val_loss': 2820.639892578125}, {'val_loss': 2820.630859375}, {'val_loss': 2820.637939453125}, {'val_loss': 2820.625244140625}, {'val_loss': 2820.616943359375}, {'val_loss': 2820.618408203125}, {'val_loss': 2820.606689453125}, {'val_loss': 2820.607666015625}, {'val_loss': 2820.607421875}, {'val_loss': 2820.64453125}, {'val_loss': 2820.640625}, {'val_loss': 2820.646240234375}, {'val_loss': 2820.6181640625}, {'val_loss': 2820.591796875}, {'val_loss': 2820.610107421875}, {'val_loss': 2820.582275390625}, {'val_loss': 2820.6025390625}, {'val_loss': 2820.583251953125}, {'val_loss': 2820.579833984375}, {'val_loss': 2820.545654296875}, {'val_loss': 2820.552734375}, {'val_loss': 2820.553466796875}, {'val_loss': 2820.572509765625}, {'val_loss': 2820.576171875}, {'val_loss': 2820.593017578125}, {'val_loss': 2820.587890625}, {'val_loss': 2820.575927734375}, {'val_loss': 2820.578125}, {'val_loss': 2820.571044921875}, {'val_loss': 2820.5498046875}, {'val_loss': 2820.5634765625}, {'val_loss': 2820.579345703125}, {'val_loss': 2820.567138671875}, {'val_loss': 2820.538330078125}, {'val_loss': 2820.51953125}, {'val_loss': 2820.552978515625}, {'val_loss': 2820.5498046875}, {'val_loss': 2820.534912109375}, {'val_loss': 2820.543701171875}, {'val_loss': 2820.495849609375}, {'val_loss': 2820.516845703125}, {'val_loss': 2820.522705078125}, {'val_loss': 2820.447265625}, {'val_loss': 2820.465087890625}, {'val_loss': 2820.473876953125}, {'val_loss': 2820.4833984375}, {'val_loss': 2820.505615234375}, {'val_loss': 2820.5302734375}, {'val_loss': 2820.538330078125}, {'val_loss': 2820.551025390625}, {'val_loss': 2820.554443359375}, {'val_loss': 2820.535400390625}, {'val_loss': 2820.518798828125}, {'val_loss': 2820.506103515625}, {'val_loss': 2820.4814453125}, {'val_loss': 2820.466796875}, {'val_loss': 2820.465576171875}, {'val_loss': 2820.453125}, {'val_loss': 2820.429443359375}, {'val_loss': 2820.433837890625}, {'val_loss': 2820.461669921875}, {'val_loss': 2820.455078125}, {'val_loss': 2820.460205078125}, {'val_loss': 2820.46484375}, {'val_loss': 2820.473876953125}, {'val_loss': 2820.463623046875}, {'val_loss': 2820.453857421875}, {'val_loss': 2820.488037109375}, {'val_loss': 2820.48046875}, {'val_loss': 2820.4658203125}, {'val_loss': 2820.455810546875}, {'val_loss': 2820.461669921875}, {'val_loss': 2820.489501953125}, {'val_loss': 2820.464599609375}, {'val_loss': 2820.468505859375}, {'val_loss': 2820.4541015625}, {'val_loss': 2820.437744140625}, {'val_loss': 2820.414794921875}, {'val_loss': 2820.417724609375}, {'val_loss': 2820.403564453125}, {'val_loss': 2820.386962890625}, {'val_loss': 2820.367431640625}, {'val_loss': 2820.34765625}, {'val_loss': 2820.366943359375}, {'val_loss': 2820.364013671875}, {'val_loss': 2820.359130859375}, {'val_loss': 2820.379150390625}, {'val_loss': 2820.407958984375}, {'val_loss': 2820.417724609375}, {'val_loss': 2820.396728515625}, {'val_loss': 2820.398193359375}, {'val_loss': 2820.4228515625}, {'val_loss': 2820.392333984375}, {'val_loss': 2820.375732421875}, {'val_loss': 2820.373046875}, {'val_loss': 2820.348876953125}, {'val_loss': 2820.348388671875}, {'val_loss': 2820.354248046875}, {'val_loss': 2820.350341796875}, {'val_loss': 2820.352783203125}, {'val_loss': 2820.34375}, {'val_loss': 2820.380126953125}, {'val_loss': 2820.384033203125}, {'val_loss': 2820.379638671875}, {'val_loss': 2820.3671875}, {'val_loss': 2820.361328125}, {'val_loss': 2820.34765625}, {'val_loss': 2820.347900390625}, {'val_loss': 2820.325439453125}, {'val_loss': 2820.335693359375}, {'val_loss': 2820.330810546875}, {'val_loss': 2820.331787109375}, {'val_loss': 2820.333251953125}, {'val_loss': 2820.340576171875}, {'val_loss': 2820.352783203125}, {'val_loss': 2820.352294921875}, {'val_loss': 2820.355224609375}, {'val_loss': 2820.341796875}, {'val_loss': 2820.333251953125}, {'val_loss': 2820.315673828125}, {'val_loss': 2820.294921875}, {'val_loss': 2820.279296875}, {'val_loss': 2820.27734375}, {'val_loss': 2820.287353515625}, {'val_loss': 2820.262451171875}, {'val_loss': 2820.257568359375}, {'val_loss': 2820.275146484375}, {'val_loss': 2820.2470703125}, {'val_loss': 2820.2333984375}, {'val_loss': 2820.253173828125}, {'val_loss': 2820.253173828125}, {'val_loss': 2820.249267578125}, {'val_loss': 2820.283203125}, {'val_loss': 2820.280029296875}, {'val_loss': 2820.25390625}, {'val_loss': 2820.222412109375}, {'val_loss': 2820.237060546875}, {'val_loss': 2820.2431640625}, {'val_loss': 2820.2080078125}, {'val_loss': 2820.203857421875}, {'val_loss': 2820.190185546875}, {'val_loss': 2820.1787109375}, {'val_loss': 2820.214599609375}, {'val_loss': 2820.2041015625}, {'val_loss': 2820.2109375}, {'val_loss': 2820.216064453125}, {'val_loss': 2820.185302734375}, {'val_loss': 2820.162353515625}, {'val_loss': 2820.140869140625}, {'val_loss': 2820.140625}, {'val_loss': 2820.129638671875}, {'val_loss': 2820.151123046875}, {'val_loss': 2820.16796875}, {'val_loss': 2820.15625}, {'val_loss': 2820.143310546875}, {'val_loss': 2820.169921875}, {'val_loss': 2820.144775390625}, {'val_loss': 2820.173828125}, {'val_loss': 2820.172607421875}, {'val_loss': 2820.161376953125}, {'val_loss': 2820.1708984375}, {'val_loss': 2820.1728515625}, {'val_loss': 2820.153564453125}, {'val_loss': 2820.169677734375}, {'val_loss': 2820.172119140625}, {'val_loss': 2820.146240234375}, {'val_loss': 2820.163818359375}, {'val_loss': 2820.154296875}, {'val_loss': 2820.152099609375}, {'val_loss': 2820.148681640625}, {'val_loss': 2820.1337890625}, {'val_loss': 2820.127197265625}, {'val_loss': 2820.114013671875}, {'val_loss': 2820.103515625}, {'val_loss': 2820.128173828125}, {'val_loss': 2820.102783203125}, {'val_loss': 2820.079345703125}, {'val_loss': 2820.053466796875}, {'val_loss': 2820.052734375}, {'val_loss': 2820.091064453125}, {'val_loss': 2820.086669921875}, {'val_loss': 2820.066162109375}, {'val_loss': 2820.074462890625}, {'val_loss': 2820.085693359375}, {'val_loss': 2820.072021484375}, {'val_loss': 2820.076904296875}, {'val_loss': 2820.062744140625}, {'val_loss': 2820.0537109375}, {'val_loss': 2820.032470703125}, {'val_loss': 2820.0185546875}, {'val_loss': 2820.025634765625}, {'val_loss': 2820.035888671875}, {'val_loss': 2820.030029296875}, {'val_loss': 2820.016357421875}, {'val_loss': 2820.031982421875}, {'val_loss': 2820.009765625}, {'val_loss': 2819.9921875}, {'val_loss': 2819.977783203125}, {'val_loss': 2819.980712890625}, {'val_loss': 2819.977294921875}, {'val_loss': 2820.003173828125}, {'val_loss': 2820.012939453125}, {'val_loss': 2820.003662109375}, {'val_loss': 2820.0029296875}, {'val_loss': 2819.989013671875}, {'val_loss': 2819.9677734375}, {'val_loss': 2819.986572265625}, {'val_loss': 2819.974365234375}, {'val_loss': 2819.9736328125}, {'val_loss': 2819.984130859375}, {'val_loss': 2819.989990234375}, {'val_loss': 2820.000732421875}, {'val_loss': 2820.030517578125}, {'val_loss': 2820.0302734375}, {'val_loss': 2820.0234375}, {'val_loss': 2820.03125}, {'val_loss': 2820.0068359375}, {'val_loss': 2819.979736328125}, {'val_loss': 2819.9873046875}, {'val_loss': 2819.9892578125}, {'val_loss': 2819.955078125}, {'val_loss': 2819.944091796875}, {'val_loss': 2819.930419921875}, {'val_loss': 2819.967041015625}, {'val_loss': 2819.9404296875}, {'val_loss': 2819.957763671875}, {'val_loss': 2819.954345703125}, {'val_loss': 2819.953125}, {'val_loss': 2819.931884765625}, {'val_loss': 2819.921875}, {'val_loss': 2819.917724609375}, {'val_loss': 2819.934814453125}, {'val_loss': 2819.9169921875}, {'val_loss': 2819.933837890625}, {'val_loss': 2819.917724609375}, {'val_loss': 2819.937255859375}, {'val_loss': 2819.921875}, {'val_loss': 2819.925048828125}, {'val_loss': 2819.931884765625}, {'val_loss': 2819.913330078125}, {'val_loss': 2819.890869140625}, {'val_loss': 2819.910400390625}, {'val_loss': 2819.886474609375}, {'val_loss': 2819.875244140625}, {'val_loss': 2819.903564453125}, {'val_loss': 2819.883544921875}, {'val_loss': 2819.861572265625}, {'val_loss': 2819.841796875}, {'val_loss': 2819.819580078125}, {'val_loss': 2819.822509765625}, {'val_loss': 2819.807861328125}, {'val_loss': 2819.821533203125}, {'val_loss': 2819.806884765625}, {'val_loss': 2819.802001953125}, {'val_loss': 2819.840576171875}, {'val_loss': 2819.809326171875}, {'val_loss': 2819.790283203125}, {'val_loss': 2819.803466796875}, {'val_loss': 2819.780029296875}, {'val_loss': 2819.789794921875}, {'val_loss': 2819.7802734375}, {'val_loss': 2819.798095703125}, {'val_loss': 2819.806396484375}, {'val_loss': 2819.839111328125}, {'val_loss': 2819.853515625}, {'val_loss': 2819.841796875}, {'val_loss': 2819.825439453125}, {'val_loss': 2819.850341796875}, {'val_loss': 2819.8271484375}, {'val_loss': 2819.842529296875}, {'val_loss': 2819.812255859375}, {'val_loss': 2819.790771484375}, {'val_loss': 2819.818115234375}, {'val_loss': 2819.808349609375}, {'val_loss': 2819.7861328125}, {'val_loss': 2819.771728515625}, {'val_loss': 2819.7900390625}, {'val_loss': 2819.786376953125}, {'val_loss': 2819.77734375}, {'val_loss': 2819.763671875}, {'val_loss': 2819.740966796875}, {'val_loss': 2819.721923828125}, {'val_loss': 2819.737060546875}, {'val_loss': 2819.724609375}, {'val_loss': 2819.707763671875}, {'val_loss': 2819.7109375}, {'val_loss': 2819.744384765625}, {'val_loss': 2819.748046875}, {'val_loss': 2819.778564453125}, {'val_loss': 2819.759033203125}, {'val_loss': 2819.754638671875}, {'val_loss': 2819.783935546875}, {'val_loss': 2819.758544921875}, {'val_loss': 2819.75390625}, {'val_loss': 2819.7236328125}, {'val_loss': 2819.7509765625}, {'val_loss': 2819.732177734375}, {'val_loss': 2819.72265625}, {'val_loss': 2819.739501953125}, {'val_loss': 2819.788330078125}, {'val_loss': 2819.785888671875}, {'val_loss': 2819.805419921875}, {'val_loss': 2819.796875}, {'val_loss': 2819.8046875}, {'val_loss': 2819.7998046875}, {'val_loss': 2819.791748046875}, {'val_loss': 2819.800537109375}, {'val_loss': 2819.788330078125}, {'val_loss': 2819.774658203125}, {'val_loss': 2819.743408203125}, {'val_loss': 2819.775390625}, {'val_loss': 2819.76171875}, {'val_loss': 2819.732421875}, {'val_loss': 2819.707275390625}, {'val_loss': 2819.739013671875}, {'val_loss': 2819.73828125}, {'val_loss': 2819.7041015625}, {'val_loss': 2819.67578125}, {'val_loss': 2819.664306640625}, {'val_loss': 2819.6728515625}, {'val_loss': 2819.6396484375}, {'val_loss': 2819.631591796875}, {'val_loss': 2819.61328125}, {'val_loss': 2819.597900390625}, {'val_loss': 2819.59765625}, {'val_loss': 2819.605224609375}, {'val_loss': 2819.625244140625}, {'val_loss': 2819.6728515625}, {'val_loss': 2819.660888671875}, {'val_loss': 2819.659912109375}, {'val_loss': 2819.635498046875}, {'val_loss': 2819.637939453125}, {'val_loss': 2819.621337890625}, {'val_loss': 2819.619873046875}, {'val_loss': 2819.607421875}, {'val_loss': 2819.623779296875}, {'val_loss': 2819.616455078125}, {'val_loss': 2819.618408203125}, {'val_loss': 2819.61328125}, {'val_loss': 2819.600341796875}, {'val_loss': 2819.583740234375}, {'val_loss': 2819.578857421875}, {'val_loss': 2819.573974609375}, {'val_loss': 2819.5771484375}, {'val_loss': 2819.558349609375}, {'val_loss': 2819.54296875}, {'val_loss': 2819.563720703125}, {'val_loss': 2819.544921875}, {'val_loss': 2819.563720703125}, {'val_loss': 2819.548095703125}, {'val_loss': 2819.557373046875}, {'val_loss': 2819.545654296875}, {'val_loss': 2819.511474609375}, {'val_loss': 2819.498046875}, {'val_loss': 2819.502685546875}, {'val_loss': 2819.510498046875}, {'val_loss': 2819.510986328125}, {'val_loss': 2819.528076171875}, {'val_loss': 2819.581787109375}, {'val_loss': 2819.580322265625}, {'val_loss': 2819.57421875}, {'val_loss': 2819.574462890625}, {'val_loss': 2819.548095703125}, {'val_loss': 2819.561767578125}, {'val_loss': 2819.546630859375}, {'val_loss': 2819.533203125}, {'val_loss': 2819.531982421875}, {'val_loss': 2819.535400390625}, {'val_loss': 2819.505126953125}, {'val_loss': 2819.491943359375}, {'val_loss': 2819.538818359375}, {'val_loss': 2819.533203125}, {'val_loss': 2819.513916015625}, {'val_loss': 2819.546630859375}, {'val_loss': 2819.55078125}, {'val_loss': 2819.5361328125}, {'val_loss': 2819.512939453125}, {'val_loss': 2819.500732421875}, {'val_loss': 2819.476318359375}, {'val_loss': 2819.45703125}, {'val_loss': 2819.4619140625}, {'val_loss': 2819.4521484375}, {'val_loss': 2819.477783203125}, {'val_loss': 2819.5146484375}, {'val_loss': 2819.491943359375}, {'val_loss': 2819.480224609375}, {'val_loss': 2819.463623046875}, {'val_loss': 2819.478759765625}, {'val_loss': 2819.458251953125}, {'val_loss': 2819.478271484375}, {'val_loss': 2819.451416015625}, {'val_loss': 2819.4541015625}, {'val_loss': 2819.4404296875}, {'val_loss': 2819.4375}, {'val_loss': 2819.410400390625}, {'val_loss': 2819.421630859375}, {'val_loss': 2819.4248046875}, {'val_loss': 2819.447509765625}, {'val_loss': 2819.430419921875}, {'val_loss': 2819.40625}, {'val_loss': 2819.412353515625}, {'val_loss': 2819.405517578125}, {'val_loss': 2819.397705078125}, {'val_loss': 2819.385498046875}, {'val_loss': 2819.398193359375}, {'val_loss': 2819.376708984375}, {'val_loss': 2819.377685546875}, {'val_loss': 2819.372802734375}, {'val_loss': 2819.388916015625}, {'val_loss': 2819.4033203125}, {'val_loss': 2819.411865234375}, {'val_loss': 2819.40625}, {'val_loss': 2819.386962890625}, {'val_loss': 2819.396484375}, {'val_loss': 2819.390380859375}, {'val_loss': 2819.384765625}, {'val_loss': 2819.397705078125}, {'val_loss': 2819.404296875}, {'val_loss': 2819.427978515625}, {'val_loss': 2819.419189453125}, {'val_loss': 2819.419677734375}, {'val_loss': 2819.418212890625}, {'val_loss': 2819.409912109375}, {'val_loss': 2819.402099609375}, {'val_loss': 2819.390869140625}, {'val_loss': 2819.384765625}, {'val_loss': 2819.372314453125}, {'val_loss': 2819.349609375}, {'val_loss': 2819.348388671875}, {'val_loss': 2819.3564453125}, {'val_loss': 2819.369140625}, {'val_loss': 2819.359619140625}, {'val_loss': 2819.370361328125}, {'val_loss': 2819.353759765625}, {'val_loss': 2819.335693359375}, {'val_loss': 2819.340087890625}, {'val_loss': 2819.351318359375}, {'val_loss': 2819.379150390625}, {'val_loss': 2819.365234375}, {'val_loss': 2819.342529296875}, {'val_loss': 2819.331787109375}, {'val_loss': 2819.310791015625}, {'val_loss': 2819.304931640625}, {'val_loss': 2819.28515625}, {'val_loss': 2819.294189453125}, {'val_loss': 2819.267578125}, {'val_loss': 2819.265869140625}, {'val_loss': 2819.2685546875}, {'val_loss': 2819.252685546875}, {'val_loss': 2819.258544921875}, {'val_loss': 2819.2412109375}, {'val_loss': 2819.267333984375}, {'val_loss': 2819.244873046875}, {'val_loss': 2819.267822265625}, {'val_loss': 2819.28515625}, {'val_loss': 2819.3232421875}, {'val_loss': 2819.326171875}, {'val_loss': 2819.308837890625}, {'val_loss': 2819.310791015625}, {'val_loss': 2819.287841796875}, {'val_loss': 2819.3056640625}, {'val_loss': 2819.302001953125}, {'val_loss': 2819.310546875}, {'val_loss': 2819.308349609375}, {'val_loss': 2819.30078125}, {'val_loss': 2819.2841796875}, {'val_loss': 2819.287109375}, {'val_loss': 2819.269775390625}, {'val_loss': 2819.253662109375}, {'val_loss': 2819.2763671875}, {'val_loss': 2819.263671875}, {'val_loss': 2819.256103515625}, {'val_loss': 2819.234375}, {'val_loss': 2819.251953125}, {'val_loss': 2819.256591796875}, {'val_loss': 2819.258056640625}, {'val_loss': 2819.238037109375}, {'val_loss': 2819.2373046875}, {'val_loss': 2819.245361328125}, {'val_loss': 2819.239501953125}, {'val_loss': 2819.218017578125}, {'val_loss': 2819.219482421875}, {'val_loss': 2819.204345703125}, {'val_loss': 2819.1884765625}, {'val_loss': 2819.163330078125}, {'val_loss': 2819.163818359375}, {'val_loss': 2819.176513671875}, {'val_loss': 2819.1875}, {'val_loss': 2819.178466796875}, {'val_loss': 2819.192626953125}, {'val_loss': 2819.1943359375}, {'val_loss': 2819.164794921875}, {'val_loss': 2819.1533203125}, {'val_loss': 2819.158935546875}, {'val_loss': 2819.166015625}, {'val_loss': 2819.189453125}, {'val_loss': 2819.187744140625}, {'val_loss': 2819.169189453125}, {'val_loss': 2819.180908203125}, {'val_loss': 2819.157470703125}, {'val_loss': 2819.131591796875}, {'val_loss': 2819.16015625}, {'val_loss': 2819.1396484375}, {'val_loss': 2819.156005859375}, {'val_loss': 2819.1298828125}, {'val_loss': 2819.143798828125}, {'val_loss': 2819.140869140625}, {'val_loss': 2819.1416015625}, {'val_loss': 2819.1669921875}, {'val_loss': 2819.140869140625}, {'val_loss': 2819.130615234375}, {'val_loss': 2819.156005859375}, {'val_loss': 2819.177978515625}, {'val_loss': 2819.178466796875}, {'val_loss': 2819.15234375}, {'val_loss': 2819.138671875}, {'val_loss': 2819.131591796875}, {'val_loss': 2819.117431640625}, {'val_loss': 2819.099609375}, {'val_loss': 2819.089111328125}, {'val_loss': 2819.100341796875}, {'val_loss': 2819.07421875}, {'val_loss': 2819.091552734375}, {'val_loss': 2819.084716796875}, {'val_loss': 2819.089111328125}, {'val_loss': 2819.099853515625}, {'val_loss': 2819.105712890625}, {'val_loss': 2819.120849609375}, {'val_loss': 2819.126220703125}, {'val_loss': 2819.103271484375}, {'val_loss': 2819.092529296875}, {'val_loss': 2819.0693359375}, {'val_loss': 2819.0966796875}, {'val_loss': 2819.084228515625}, {'val_loss': 2819.132568359375}, {'val_loss': 2819.108642578125}, {'val_loss': 2819.1220703125}, {'val_loss': 2819.156982421875}, {'val_loss': 2819.150146484375}, {'val_loss': 2819.155029296875}, {'val_loss': 2819.138916015625}, {'val_loss': 2819.115966796875}, {'val_loss': 2819.101318359375}, {'val_loss': 2819.092529296875}, {'val_loss': 2819.1015625}, {'val_loss': 2819.1015625}, {'val_loss': 2819.0986328125}, {'val_loss': 2819.074462890625}, {'val_loss': 2819.083984375}, {'val_loss': 2819.065185546875}, {'val_loss': 2819.036376953125}, {'val_loss': 2819.060791015625}, {'val_loss': 2819.063232421875}, {'val_loss': 2819.082763671875}, {'val_loss': 2819.080810546875}, {'val_loss': 2819.0703125}, {'val_loss': 2819.089599609375}, {'val_loss': 2819.078125}, {'val_loss': 2819.065185546875}, {'val_loss': 2819.051513671875}, {'val_loss': 2819.025390625}, {'val_loss': 2819.032958984375}, {'val_loss': 2818.998291015625}, {'val_loss': 2818.982421875}, {'val_loss': 2818.988525390625}, {'val_loss': 2818.987548828125}, {'val_loss': 2818.961181640625}, {'val_loss': 2818.963134765625}, {'val_loss': 2818.949462890625}, {'val_loss': 2818.9560546875}, {'val_loss': 2818.955810546875}, {'val_loss': 2818.943115234375}, {'val_loss': 2818.940185546875}, {'val_loss': 2818.955322265625}, {'val_loss': 2818.970947265625}, {'val_loss': 2818.942138671875}, {'val_loss': 2818.9462890625}, {'val_loss': 2818.942626953125}, {'val_loss': 2818.966064453125}, {'val_loss': 2818.959716796875}, {'val_loss': 2818.981201171875}, {'val_loss': 2818.998779296875}, {'val_loss': 2818.98828125}, {'val_loss': 2818.970703125}, {'val_loss': 2818.975341796875}, {'val_loss': 2818.992919921875}, {'val_loss': 2818.977783203125}, {'val_loss': 2818.970703125}, {'val_loss': 2818.958251953125}, {'val_loss': 2818.966796875}, {'val_loss': 2818.9765625}, {'val_loss': 2818.967041015625}, {'val_loss': 2818.9560546875}, {'val_loss': 2818.965087890625}, {'val_loss': 2818.9541015625}, {'val_loss': 2818.940185546875}, {'val_loss': 2818.944580078125}, {'val_loss': 2818.928466796875}, {'val_loss': 2818.918212890625}, {'val_loss': 2818.907958984375}, {'val_loss': 2818.896728515625}, {'val_loss': 2818.861328125}, {'val_loss': 2818.847900390625}, {'val_loss': 2818.825439453125}, {'val_loss': 2818.833251953125}, {'val_loss': 2818.8232421875}, {'val_loss': 2818.819580078125}, {'val_loss': 2818.806884765625}, {'val_loss': 2818.782470703125}, {'val_loss': 2818.785888671875}, {'val_loss': 2818.806884765625}, {'val_loss': 2818.875732421875}, {'val_loss': 2818.856201171875}, {'val_loss': 2818.8447265625}, {'val_loss': 2818.896484375}, {'val_loss': 2818.899169921875}, {'val_loss': 2818.899169921875}, {'val_loss': 2818.874267578125}, {'val_loss': 2818.883544921875}, {'val_loss': 2818.873046875}, {'val_loss': 2818.870849609375}, {'val_loss': 2818.876708984375}, {'val_loss': 2818.875732421875}, {'val_loss': 2818.8623046875}, {'val_loss': 2818.923828125}, {'val_loss': 2818.947265625}, {'val_loss': 2818.934326171875}, {'val_loss': 2818.924072265625}, {'val_loss': 2818.920654296875}, {'val_loss': 2818.894775390625}, {'val_loss': 2818.893310546875}, {'val_loss': 2818.867919921875}, {'val_loss': 2818.8564453125}, {'val_loss': 2818.853515625}, {'val_loss': 2818.8427734375}, {'val_loss': 2818.823974609375}, {'val_loss': 2818.843505859375}, {'val_loss': 2818.8330078125}, {'val_loss': 2818.823974609375}, {'val_loss': 2818.8212890625}, {'val_loss': 2818.8046875}, {'val_loss': 2818.792724609375}, {'val_loss': 2818.805419921875}, {'val_loss': 2818.840087890625}, {'val_loss': 2818.823974609375}, {'val_loss': 2818.841064453125}, {'val_loss': 2818.852783203125}, {'val_loss': 2818.8388671875}, {'val_loss': 2818.825927734375}, {'val_loss': 2818.80078125}, {'val_loss': 2818.792236328125}, {'val_loss': 2818.810791015625}, {'val_loss': 2818.8017578125}, {'val_loss': 2818.816650390625}, {'val_loss': 2818.798583984375}, {'val_loss': 2818.78515625}, {'val_loss': 2818.8125}, {'val_loss': 2818.814453125}, {'val_loss': 2818.801025390625}, {'val_loss': 2818.774169921875}, {'val_loss': 2818.749267578125}, {'val_loss': 2818.744140625}, {'val_loss': 2818.718017578125}, {'val_loss': 2818.77734375}, {'val_loss': 2818.750732421875}, {'val_loss': 2818.760986328125}, {'val_loss': 2818.73046875}, {'val_loss': 2818.707275390625}, {'val_loss': 2818.709228515625}, {'val_loss': 2818.739501953125}, {'val_loss': 2818.738525390625}, {'val_loss': 2818.725830078125}, {'val_loss': 2818.72265625}, {'val_loss': 2818.731201171875}, {'val_loss': 2818.712646484375}, {'val_loss': 2818.716796875}, {'val_loss': 2818.693603515625}, {'val_loss': 2818.716064453125}, {'val_loss': 2818.736572265625}, {'val_loss': 2818.716064453125}, {'val_loss': 2818.708984375}, {'val_loss': 2818.7099609375}, {'val_loss': 2818.688720703125}, {'val_loss': 2818.692626953125}, {'val_loss': 2818.6689453125}, {'val_loss': 2818.642578125}, {'val_loss': 2818.634521484375}, {'val_loss': 2818.616943359375}, {'val_loss': 2818.656494140625}, {'val_loss': 2818.657958984375}, {'val_loss': 2818.686279296875}, {'val_loss': 2818.687255859375}, {'val_loss': 2818.6669921875}, {'val_loss': 2818.6748046875}, {'val_loss': 2818.657470703125}, {'val_loss': 2818.657470703125}, {'val_loss': 2818.654052734375}, {'val_loss': 2818.637451171875}, {'val_loss': 2818.6728515625}, {'val_loss': 2818.656982421875}, {'val_loss': 2818.660888671875}, {'val_loss': 2818.667724609375}, {'val_loss': 2818.662109375}, {'val_loss': 2818.658935546875}, {'val_loss': 2818.6171875}, {'val_loss': 2818.644775390625}, {'val_loss': 2818.664794921875}, {'val_loss': 2818.688720703125}, {'val_loss': 2818.694091796875}, {'val_loss': 2818.716552734375}, {'val_loss': 2818.692626953125}, {'val_loss': 2818.6904296875}, {'val_loss': 2818.684326171875}, {'val_loss': 2818.683837890625}, {'val_loss': 2818.671875}, {'val_loss': 2818.654541015625}, {'val_loss': 2818.628662109375}, {'val_loss': 2818.618408203125}, {'val_loss': 2818.6103515625}, {'val_loss': 2818.610107421875}, {'val_loss': 2818.646728515625}, {'val_loss': 2818.666748046875}, {'val_loss': 2818.671875}, {'val_loss': 2818.651611328125}, {'val_loss': 2818.625}, {'val_loss': 2818.6181640625}, {'val_loss': 2818.609130859375}, {'val_loss': 2818.603759765625}, {'val_loss': 2818.5908203125}, {'val_loss': 2818.595703125}, {'val_loss': 2818.5751953125}, {'val_loss': 2818.580322265625}, {'val_loss': 2818.560546875}, {'val_loss': 2818.5712890625}, {'val_loss': 2818.567138671875}, {'val_loss': 2818.558349609375}, {'val_loss': 2818.556884765625}, {'val_loss': 2818.5634765625}, {'val_loss': 2818.565185546875}, {'val_loss': 2818.568115234375}, {'val_loss': 2818.57421875}, {'val_loss': 2818.580322265625}, {'val_loss': 2818.552001953125}, {'val_loss': 2818.5869140625}, {'val_loss': 2818.571533203125}, {'val_loss': 2818.593017578125}, {'val_loss': 2818.585693359375}, {'val_loss': 2818.5888671875}, {'val_loss': 2818.559326171875}, {'val_loss': 2818.532958984375}, {'val_loss': 2818.541748046875}, {'val_loss': 2818.5283203125}, {'val_loss': 2818.509033203125}, {'val_loss': 2818.501708984375}, {'val_loss': 2818.509765625}, {'val_loss': 2818.516845703125}, {'val_loss': 2818.524658203125}, {'val_loss': 2818.560791015625}, {'val_loss': 2818.54296875}, {'val_loss': 2818.5380859375}, {'val_loss': 2818.544677734375}, {'val_loss': 2818.536376953125}, {'val_loss': 2818.576904296875}, {'val_loss': 2818.567138671875}, {'val_loss': 2818.565673828125}, {'val_loss': 2818.5537109375}, {'val_loss': 2818.581787109375}, {'val_loss': 2818.567138671875}, {'val_loss': 2818.5625}, {'val_loss': 2818.576171875}, {'val_loss': 2818.555419921875}, {'val_loss': 2818.551025390625}, {'val_loss': 2818.5478515625}, {'val_loss': 2818.5302734375}, {'val_loss': 2818.504638671875}, {'val_loss': 2818.4931640625}, {'val_loss': 2818.4677734375}, {'val_loss': 2818.4443359375}, {'val_loss': 2818.4072265625}, {'val_loss': 2818.3896484375}, {'val_loss': 2818.4287109375}, {'val_loss': 2818.460693359375}, {'val_loss': 2818.451416015625}, {'val_loss': 2818.4521484375}, {'val_loss': 2818.451904296875}, {'val_loss': 2818.453369140625}, {'val_loss': 2818.454345703125}, {'val_loss': 2818.455078125}, {'val_loss': 2818.4560546875}, {'val_loss': 2818.456298828125}, {'val_loss': 2818.453857421875}, {'val_loss': 2818.452392578125}, {'val_loss': 2818.451904296875}, {'val_loss': 2818.451171875}, {'val_loss': 2818.4501953125}, {'val_loss': 2818.4482421875}, {'val_loss': 2818.447265625}, {'val_loss': 2818.447265625}, {'val_loss': 2818.4482421875}, {'val_loss': 2818.447998046875}, {'val_loss': 2818.448486328125}, {'val_loss': 2818.44921875}, {'val_loss': 2818.451416015625}, {'val_loss': 2818.450439453125}, {'val_loss': 2818.451171875}, {'val_loss': 2818.451416015625}, {'val_loss': 2818.449951171875}, {'val_loss': 2818.44921875}, {'val_loss': 2818.448486328125}, {'val_loss': 2818.450439453125}, {'val_loss': 2818.450439453125}, {'val_loss': 2818.44921875}, {'val_loss': 2818.4501953125}, {'val_loss': 2818.450927734375}, {'val_loss': 2818.453857421875}, {'val_loss': 2818.451904296875}, {'val_loss': 2818.450439453125}, {'val_loss': 2818.450927734375}, {'val_loss': 2818.452392578125}, {'val_loss': 2818.451904296875}, {'val_loss': 2818.452392578125}, {'val_loss': 2818.451171875}, {'val_loss': 2818.449951171875}, {'val_loss': 2818.448486328125}, {'val_loss': 2818.447265625}, {'val_loss': 2818.446044921875}, {'val_loss': 2818.446044921875}, {'val_loss': 2818.448974609375}, {'val_loss': 2818.447265625}, {'val_loss': 2818.447509765625}, {'val_loss': 2818.4453125}, {'val_loss': 2818.4453125}, {'val_loss': 2818.446533203125}, {'val_loss': 2818.4462890625}, {'val_loss': 2818.446044921875}, {'val_loss': 2818.447021484375}, {'val_loss': 2818.4443359375}, {'val_loss': 2818.444580078125}, {'val_loss': 2818.445068359375}, {'val_loss': 2818.444091796875}, {'val_loss': 2818.44140625}, {'val_loss': 2818.4404296875}, {'val_loss': 2818.44140625}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.439208984375}, {'val_loss': 2818.4404296875}, {'val_loss': 2818.4404296875}, {'val_loss': 2818.440185546875}, {'val_loss': 2818.4365234375}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.4375}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.4375}, {'val_loss': 2818.436279296875}, {'val_loss': 2818.438232421875}, {'val_loss': 2818.439697265625}, {'val_loss': 2818.4404296875}, {'val_loss': 2818.438720703125}, {'val_loss': 2818.4365234375}, {'val_loss': 2818.433837890625}, {'val_loss': 2818.435791015625}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.4384765625}, {'val_loss': 2818.439697265625}, {'val_loss': 2818.437744140625}, {'val_loss': 2818.433837890625}, {'val_loss': 2818.43359375}, {'val_loss': 2818.43359375}, {'val_loss': 2818.434814453125}, {'val_loss': 2818.433837890625}, {'val_loss': 2818.432861328125}, {'val_loss': 2818.433837890625}, {'val_loss': 2818.430908203125}, {'val_loss': 2818.432373046875}, {'val_loss': 2818.434326171875}, {'val_loss': 2818.435302734375}, {'val_loss': 2818.434326171875}, {'val_loss': 2818.435302734375}, {'val_loss': 2818.432861328125}, {'val_loss': 2818.4306640625}, {'val_loss': 2818.4306640625}, {'val_loss': 2818.432373046875}, {'val_loss': 2818.429931640625}, {'val_loss': 2818.430908203125}, {'val_loss': 2818.430908203125}, {'val_loss': 2818.430908203125}, {'val_loss': 2818.430419921875}, {'val_loss': 2818.4296875}, {'val_loss': 2818.431396484375}, {'val_loss': 2818.430908203125}, {'val_loss': 2818.429931640625}, {'val_loss': 2818.431396484375}, {'val_loss': 2818.4296875}, {'val_loss': 2818.4287109375}, {'val_loss': 2818.428466796875}, {'val_loss': 2818.428466796875}, {'val_loss': 2818.430419921875}, {'val_loss': 2818.430419921875}, {'val_loss': 2818.428466796875}, {'val_loss': 2818.4267578125}, {'val_loss': 2818.4248046875}, {'val_loss': 2818.424560546875}, {'val_loss': 2818.422607421875}, {'val_loss': 2818.423095703125}, {'val_loss': 2818.421142578125}, {'val_loss': 2818.419677734375}, {'val_loss': 2818.417236328125}, {'val_loss': 2818.420166015625}, {'val_loss': 2818.421630859375}, {'val_loss': 2818.423095703125}, {'val_loss': 2818.422607421875}, {'val_loss': 2818.423095703125}, {'val_loss': 2818.421875}, {'val_loss': 2818.419189453125}, {'val_loss': 2818.420654296875}, {'val_loss': 2818.419677734375}, {'val_loss': 2818.416748046875}, {'val_loss': 2818.417236328125}, {'val_loss': 2818.416259765625}, {'val_loss': 2818.416015625}, {'val_loss': 2818.4150390625}, {'val_loss': 2818.416259765625}, {'val_loss': 2818.4169921875}, {'val_loss': 2818.4169921875}, {'val_loss': 2818.418212890625}, {'val_loss': 2818.415771484375}, {'val_loss': 2818.4150390625}, {'val_loss': 2818.414306640625}, {'val_loss': 2818.414306640625}, {'val_loss': 2818.415771484375}, {'val_loss': 2818.416015625}, {'val_loss': 2818.418701171875}, {'val_loss': 2818.4189453125}, {'val_loss': 2818.420654296875}, {'val_loss': 2818.422119140625}, {'val_loss': 2818.423828125}, {'val_loss': 2818.423095703125}, {'val_loss': 2818.420166015625}, {'val_loss': 2818.418701171875}, {'val_loss': 2818.416015625}, {'val_loss': 2818.417236328125}, {'val_loss': 2818.416748046875}, {'val_loss': 2818.417724609375}, {'val_loss': 2818.4189453125}, {'val_loss': 2818.420654296875}, {'val_loss': 2818.419677734375}, {'val_loss': 2818.418212890625}, {'val_loss': 2818.417236328125}, {'val_loss': 2818.417236328125}, {'val_loss': 2818.414306640625}, {'val_loss': 2818.4140625}, {'val_loss': 2818.412109375}, {'val_loss': 2818.4130859375}, {'val_loss': 2818.415283203125}, {'val_loss': 2818.414306640625}, {'val_loss': 2818.412841796875}, {'val_loss': 2818.4150390625}, {'val_loss': 2818.4150390625}, {'val_loss': 2818.412353515625}, {'val_loss': 2818.415283203125}, {'val_loss': 2818.4140625}, {'val_loss': 2818.4130859375}, {'val_loss': 2818.4111328125}, {'val_loss': 2818.4091796875}, {'val_loss': 2818.407958984375}, {'val_loss': 2818.408935546875}, {'val_loss': 2818.408447265625}, {'val_loss': 2818.408935546875}, {'val_loss': 2818.406982421875}, {'val_loss': 2818.406494140625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405517578125}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404296875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.4052734375}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.405029296875}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404541015625}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.404296875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404296875}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.404296875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404296875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.404052734375}, {'val_loss': 2818.403564453125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.4033203125}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.40234375}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402587890625}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.402587890625}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402587890625}, {'val_loss': 2818.403076171875}, {'val_loss': 2818.402587890625}, {'val_loss': 2818.402587890625}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.400634765625}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.401123046875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.4013671875}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.40234375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.402099609375}, {'val_loss': 2818.401611328125}, {'val_loss': 2818.401611328125}]\n"
     ]
    }
   ],
   "source": [
    "val_loss = [result] + history1 + history2 + history3 + history4 + history5\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log the final validation loss to Jovian and commit the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Metrics logged.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics(val_loss=val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"soham-a-shah/02-insurance-linear-regression\" on https://jovian.ml/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Attaching records (metrics, hyperparameters, dataset etc.)\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ml/soham-a-shah/02-insurance-linear-regression\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/soham-a-shah/02-insurance-linear-regression'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scroll back up, re-initialize the model, and try different set of values for batch size, number of epochs, learning rate etc. Commit each experiment and use the \"Compare\" and \"View Diff\" options on Jovian to compare the different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions using the trained model\n",
    "\n",
    "**Q: Complete the following function definition to make predictions on a single input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(input, target, model):\n",
    "    inputs = input.unsqueeze(0)\n",
    "    predictions = model(inputs)                # fill this\n",
    "    prediction = predictions[0].detach()\n",
    "    print(\"Input:\", input)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([44.0000,  0.0000, 26.8422,  1.0000,  0.0000,  2.0000])\n",
      "Target: tensor([3740.5232])\n",
      "Prediction: tensor([4184.9575])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[0]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([19.0000,  0.0000, 23.7380,  5.0000,  0.0000,  3.0000])\n",
      "Target: tensor([2297.0205])\n",
      "Prediction: tensor([1499.9332])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[10]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([50.0000,  0.0000, 22.7088,  0.0000,  0.0000,  0.0000])\n",
      "Target: tensor([12571.7217])\n",
      "Prediction: tensor([5152.3555])\n"
     ]
    }
   ],
   "source": [
    "input, target = val_ds[23]\n",
    "predict_single(input, target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you happy with your model's predictions? Try to improve them further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Step 6: Try another dataset & blog about it\n",
    "\n",
    "While this last step is optional for the submission of your assignment, we highly recommend that you do it. Try to clean up & replicate this notebook (or [this one](https://jovian.ml/aakashns/housing-linear-minimal), or [this one](https://jovian.ml/aakashns/mnist-logistic-minimal) ) for a different linear regression or logistic regression problem. This will help solidify your understanding, and give you a chance to differentiate the generic patters in machine learning from problem-specific details.\n",
    "\n",
    "Here are some sources to find good datasets:\n",
    "\n",
    "- https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/\n",
    "- https://www.kaggle.com/rtatman/datasets-for-regression-analysis\n",
    "- https://archive.ics.uci.edu/ml/datasets.php?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table\n",
    "- https://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\n",
    "- https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "- https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "We also recommend that you write a blog about your approach to the problem. Here is a suggested structure for your post (feel free to experiment with it):\n",
    "\n",
    "- Interesting title & subtitle\n",
    "- Overview of what the blog covers (which dataset, linear regression or logistic regression, intro to PyTorch)\n",
    "- Downloading & exploring the data\n",
    "- Preparing the data for training\n",
    "- Creating a model using PyTorch\n",
    "- Training the model to fit the data\n",
    "- Your thoughts on how to experiment with different hyperparmeters to reduce loss\n",
    "- Making predictions using the model\n",
    "\n",
    "As with the previous assignment, you can [embed Juptyer notebook cells & outputs from Jovian](https://medium.com/jovianml/share-and-embed-jupyter-notebooks-online-with-jovian-ml-df709a03064e) into your blog. \n",
    "\n",
    "Don't forget to share your work on the forum: https://jovian.ml/forum/t/share-your-work-here-assignment-2/4931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(project=project_name, environment=None)\n",
    "jovian.commit(project=project_name, environment=None) # try again, kaggle fails sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
